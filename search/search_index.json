{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"contributing/","title":"Contributing","text":"<p>Issues and pull requests are more than welcome: github.com/developmentseed/eoAPI/issues</p> <p>You can also start Discussions in github.com/developmentseed/eoAPI/discussions</p>"},{"location":"contributing/#open-source","title":"Open Source","text":"<p>You can also contribute indirectly to eoAPI by helping on the sub-modules:</p> <ul> <li>PgSTAC database stac-utils/pgstac</li> <li>stac-fastapi: stac-utils/stac-fastapi</li> <li>titiler-pgstac:  stac-utils/titiler-pgstac</li> <li>TiTiler: developmentseed/titiler</li> <li>TiPg: developmentseed/tipg</li> </ul>"},{"location":"customization/","title":"Customization","text":"<p>The eoapi-devseed repository (developmentseed/eoapi-devseed) hosts customized versions of each base service. The documentation below demonstrates how each service can be customized. The eoAPI services can work in parallel or in combination with each other.</p>"},{"location":"customization/#eoapistac","title":"eoapi.stac","text":"<p>Built on stac-fastapi.pgstac application, adding a <code>TiTilerExtension</code> and a simple <code>Search Viewer</code>.</p> <p>The service includes:</p> <ul> <li>Full stac-fastapi implementation - see docs if using the <code>docker-compose</code> configuration.</li> <li>Simple STAC Search viewer - see viewer if using the <code>docker-compose</code> configuration.</li> <li>Proxy to the tiler endpoint for STAC Items.</li> </ul> <p>When the <code>TITILER_ENDPOINT</code> environment variable is set (pointing to the <code>raster</code> application), additional endpoints will be added to the stac-fastapi application (see: stac/extension.py):</p> <ul> <li><code>/collections/{collectionId}/items/{itemId}/tilejson.json</code>: Return the <code>raster</code> tilejson for an item</li> <li><code>/collections/{collectionId}/items/{itemId}/viewer</code>: Redirect to the <code>raster</code> viewer</li> </ul> <p> </p> <p>Code: /runtimes/eoapi/stac</p>"},{"location":"customization/#eoapiraster","title":"eoapi.raster","text":"<p>The dynamic tiler deployed within <code>eoapi-devseed</code> is built on top of titiler-pgstac and pgstac. It enables large-scale mosaic based on the results of STAC search queries.</p> <p>The service includes all the default endpoints from titiler-pgstac application and:</p> <ul> <li> <p><code>/</code>: a custom landing page with links to the different endpoints</p> </li> <li> <p><code>/mosaic/builder</code>: a virtual mosaic builder UI, which helps create and register STAC Search queries</p> </li> <li> <p><code>/collections</code>: a secret (not in OpenAPI documentation) endpoint used in the mosaic-builder page</p> </li> <li> <p><code>/collections/{collection_id}/items/{item_id}/viewer</code>: a simple STAC Item viewer</p> </li> </ul> <p> </p> <p>Code: /runtimes/eoapi/raster</p>"},{"location":"customization/#eoapivector","title":"eoapi.vector","text":"<p>OGC Features and Tiles API built on top of tipg.</p> <p>By default, the API will look for tables in the <code>public</code> schema of the database. We've also added three functions that connect to the pgSTAC schema:</p> <ul> <li>pg_temp.pgstac_collections_view: Simple function which returns PgSTAC Collections</li> <li>pg_temp.pgstac_hash: Return features for a specific <code>searchId</code> (hash)</li> <li>pg_temp.pgstac_hash_count: Return the number of items per geometry for a specific <code>searchId</code> (hash)</li> </ul> <p> </p> <p>Code: /runtimes/eoapi/vector</p> <p>--</p>"},{"location":"customization/#stac-browser","title":"STAC browser","text":"<p>The custom browser configuration can be modified using the config located in /dockerfiles/browser_config.js. For more information about available configurations, see the Radiant Earth repository.</p>"},{"location":"deployment/","title":"Deployment","text":"<p>eoAPI can be deployed using Kubernetes or AWS CDK.</p>"},{"location":"deployment/#kubernetes-recommended","title":"Kubernetes (Recommended)","text":"<p>Production-ready deployment with high availability, auto-scaling, and monitoring.</p> <p>\u2192 Complete Kubernetes Documentation</p> <ul> <li>Multi-cloud support (AWS, GCP, Azure)</li> <li>Helm-based installation</li> <li>Built-in monitoring and autoscaling</li> </ul>"},{"location":"deployment/#aws-cdk","title":"AWS CDK","text":"<p>Serverless deployment using AWS Lambda, RDS, and API Gateway.</p> <p>\u2192 Complete AWS CDK Documentation</p> <ul> <li>Pay-per-request pricing</li> <li>Managed infrastructure</li> <li>AWS-native integration</li> </ul>"},{"location":"intro/","title":"Intro","text":"<p> <p>Create a full Earth Observation API with Metadata, Raster, and Vector services.</p> </p>"},{"location":"intro/#earth-observation-api","title":"Earth Observation API","text":"<p><code>eoAPI</code> combines several state-of-the-art projects to create a full Earth Observation API. Each service can be used and deployed independently, but <code>eoAPI</code> creates the interconnections between each service:</p> <ul> <li> <p>pgSTAC database stac-utils/pgstac</p> </li> <li> <p>STAC API built on top of stac-utils/stac-fastapi</p> </li> <li> <p>STAC browser a UI that exposes, in a user friendly interface, the metadata served by the STAC API. Built on top of radiantearth/stac-browser</p> </li> <li> <p>STAC Items And Mosaic Raster Tiles API built on top of stac-utils/titiler-pgstac</p> </li> <li> <p>OGC Features and Vector Tiles API built on top of developmentseed/tipg</p> </li> </ul>"},{"location":"intro/#eoapi-an-open-source-community-project","title":"\ud83c\udf0d eoAPI: An Open-Source Community Project","text":"<p><code>eoAPI</code> is proudly open-source and driven by a dedicated community of contributors. We believe in the power of open collaboration and welcome anyone to contribute, discuss, and grow this tool. Join the conversations on GitHub Discussions and make a difference in the Earth Observation realm.</p>"},{"location":"intro/#why-should-you-use-eoapi","title":"Why should you use <code>eoAPI</code>","text":"<ul> <li> <p>Focus on your use case: <code>eoAPI</code>\u00a0is used for large-scale data processing, building geographic information systems (GIS), creating real-time data applications, climate research and environmental monitoring, machine learning model training, and more.</p> </li> <li> <p>Unified Repository: <code>eoAPI</code>\u00a0provides a single, unified repository for several state-of-the-art Earth Observation (EO) data services, including Metadata search (STAC), Raster, and Vector services. This can simplify the process of accessing and working with these services.</p> </li> <li> <p>Interoperability: <code>eoAPI</code>\u00a0is designed to enable interoperability among its included services. This can make building complex applications that leverage different types of EO data easier.</p> </li> <li> <p>Open Source and Community Support:\u00a0As an open-source project,\u00a0<code>eoAPI</code>\u00a0allows developers to inspect its code, contribute to its development, and use it as a base for custom solutions. It also benefits from the support and innovation of a community of developers and EO data users.</p> </li> <li> <p>Scalability and Flexibility:\u00a0Each service in\u00a0<code>eoAPI</code>\u00a0can be used or deployed independently, which provides a lot of flexibility. If a developer's application only requires one or two of eoAPI's services, they don't need to deploy the entire suite.</p> </li> <li> <p>Facilitate Earth Observation Tasks: <code>eoAPI</code>\u00a0includes specialized tools for working with EO data, such as dynamic tiling, metadata searching, and features/vector tiles API. These can significantly facilitate EO data processing, analysis, and visualization.</p> </li> <li> <p>Ease of Deployment: <code>eoAPI</code>\u00a0supports containerized deployment using Docker, making it easier to set up, scale, and maintain applications built on it. Spin up the demo locally and start experimenting in minutes.</p> </li> </ul>"},{"location":"intro/#services-overview","title":"Services Overview","text":"<ul> <li> <p>STAC Metadata: Built with stac-fastapi.pgstac to enable data discovery. See the specifications core, search and features for API details.</p> </li> <li> <p>STAC browser : Built with the Radiant Earth STAC browser to provide a simple user-friendly interface for searching the STAC metadata.</p> </li> <li> <p>Raster Tiles: Built with titiler-pgstac and pgstac to enable large scale mosaic based on results of STAC searches queries. See docs for API details.</p> </li> <li> <p>OGC Features &amp; Vector Tiles: Built with tipg to create a lightweight OGC Features and Tiles API with a PostGIS database. See docs for API details.</p> </li> </ul> <p>See service details for more information.</p>"},{"location":"intro/#getting-started","title":"Getting started","text":"<p>The easiest way to start exploring the different eoAPI services is with Docker. Clone this repository and start the multi-container Docker applications using <code>Compose</code>:</p> <pre><code>git clone https://github.com/developmentseed/eoAPI.git\ncd eoAPI\ndocker compose up\n</code></pre> <p>Once the applications are up, you'll need to add STAC Collections and Items to the PgSTAC database. If you don't have, you can use the follow the MAXAR open data demo (or get inspired by the other demos).</p> <p>Then you can start exploring your dataset with:</p> <ul> <li>the STAC Metadata service http://localhost:8081</li> <li>the Raster service http://localhost:8082</li> </ul> <p>Info</p> <p>If you've added vector datasets to the <code>public</code> schema in the Postgres database, they will be available through the Vector service at http://localhost:8083.</p> <p>Alternatively, you may install and launch applications locally:</p> <pre><code>python -m pip install --upgrade virtualenv\nvirtualenv .venv\nsource .venv/bin/activate\n\nexport DATABASE_URL=postgresql://username:password@0.0.0.0:5439/postgis  # Connect to the database of your choice\n\npython -m pip install uvicorn\n\n###############################################################################\n# Install and launch the application\n# Select one of the following\n\n###############################################################################\n# STAC\npython -m pip install \"psycopg[binary,pool]\" stac-fastapi-pgstac\npython -m uvicorn stac_fastapi.pgstac.app:app --port 8081 --reload\n\n###############################################################################\n# RASTER\npython -m pip install \"psycopg[binary,pool]\" titiler-pgstac\npython -m uvicorn titiler.pgstac.main:app --port 8082 --reload\n\n###############################################################################\n# VECTOR\npython -m pip install tipg\npython -m uvicorn tipg.main:app --port 8083 --reload\n</code></pre> <p>Danger</p> <p>Python applications might have incompatible dependencies, which you can resolve by using a virtual environment per application</p>"},{"location":"services/","title":"Services","text":"<p>eoAPI combines several state-of-the-art projects to create an entire Earth Observation API. Each service can be used and deployed independently, but eoAPI creates the interconnections between each service:</p> <ul> <li>pgSTAC database stac-utils/pgstac</li> <li>STAC API built on top of stac-utils/stac-fastapi</li> <li>STAC Items And Mosaic Raster Tiles API built on top of stac-utils/titiler-pgstac</li> <li>OGC Features and Vector Tiles API built on top of developmentseed/tipg</li> </ul>"},{"location":"services/#database","title":"Database","text":"<p>The STAC database is at the heart of eoAPI and is the only mandatory service. We use <code>PgSTAC</code> Postgres schema and functions, which provides functionality for STAC Filters, CQL2 search, and utilities to help manage the indexing and partitioning of STAC Collections and Items.</p> <p>PgSTAC is used in production to scale to hundreds of millions of STAC items. PgSTAC implements core data models and functions to provide a STAC API from a PostgreSQL database. PgSTAC is entirely within the database and does not provide an HTTP-facing API. The Stac FastAPI PgSTAC backend and Franklin can be used to expose a PgSTAC catalog. Integrating PgSTAC with any other language with PostgreSQL drivers is also possible.</p> <p>PgSTAC Documentation: stac-utils.github.io/pgstac/pgstac</p> <p>pyPgSTAC Documentation: stac-utils.github.io/pgstac/pypgstac</p>"},{"location":"services/#metadata","title":"Metadata","text":"<p>The Metadata service deployed in eoAPI is built on stac-fastapi.pgstac application.</p> <p>By default, the STAC metadata service will have a set of endpoints to search and list STAC collections and items.</p> <p> </p> <p>Example</p> <ul> <li> <p>stac.eoapi.dev landing page</p> </li> <li> <p>stac.eoapi.dev/collections list available Collection</p> </li> <li> <p>stac.eoapi.dev/collections/MAXAR_southafrica_flooding22/items list available Items for the <code>MAXAR_southafrica_flooding22</code> collection</p> </li> <li> <p>stac.eoapi.dev/collections/MAXAR_southafrica_flooding22/items/36_213131033000_1040010076566100 get <code>36_213131033000_1040010076566100</code> Item in the <code>MAXAR_southafrica_flooding22</code> collection</p> </li> <li> <p>stac.eoapi.dev/search list of Items in the catalog</p> </li> <li> <p>stac.eoapi.dev/search?collections=MAXAR_Kahramanmaras_turkey_earthquake_23&amp;limit=5&amp;datetime=2023-02-06T00:00:00Z/2023-02-10T00:00:00Z list of Items in the catalog for the <code>MAXAR_Kahramanmaras_turkey_earthquake_23</code> collection between February 6<sup>th</sup> and 10<sup>th</sup></p> </li> </ul>"},{"location":"services/#raster","title":"Raster","text":"<p>The Raster service deployed in <code>eoAPI</code> is built on top of titiler-pgstac.</p> <p>It enables Raster visualization for a single STAC Item and large-scale (multi collections/items) mosaic based on STAC search queries.</p> <p> </p> <p>Example</p> <ul> <li>raster.eoapi.dev landing page</li> </ul> <p>Items endpoints</p> <ul> <li> <p>raster.eoapi.dev/collections/MAXAR_southafrica_flooding22/items/36_213131033000_1040010076566100/info get Raster metadata information about Assets found in <code>36_213131033000_1040010076566100</code> Item in the <code>MAXAR_southafrica_flooding22</code> collection</p> </li> <li> <p>raster.eoapi.dev/collections/MAXAR_southafrica_flooding22/items/36_213131033000_1040010076566100/info?assets=visual get Raster metadata information only for the <code>visual</code> Asset</p> </li> <li> <p>raster.eoapi.dev/collections/MAXAR_southafrica_flooding22/items/36_213131033000_1040010076566100/map?assets=visual&amp;minzoom=12&amp;maxzoom=19 show the <code>visual</code> Asset on a Map client</p> </li> <li> <p>raster.eoapi.dev/collections/MAXAR_southafrica_flooding22/items/36_213131033000_1040010076566100/tilejson.json?assets=ms_analytic&amp;minzoom=13&amp;maxzoom=17&amp;asset_bidx=ms_analytic|8,2,1&amp;rescale=0,4000 get a TileJSON document for the  <code>ms_analytic</code> Asset with band combination 8,2,1 with values rescaling from 0,4000 to 0,255</p> </li> </ul> <p>Mosaic endpoints</p> <ul> <li> <p>raster.eoapi.dev/searches/list list pre-registered Virtual Mosaics</p> </li> <li> <p>raster.eoapi.dev/searches/2f3073257a5b6530aedbb0e4b4f726fa/info get information about the <code>2f3073257a5b6530aedbb0e4b4f726fa</code> mosaic</p> </li> <li> <p>raster.eoapi.dev/searches/2f3073257a5b6530aedbb0e4b4f726fa/map?assets=visual&amp;minzoom=12&amp;maxzoom=19 show the <code>2f3073257a5b6530aedbb0e4b4f726fa</code> mosaic and using the <code>visual</code> Asset on a Map client</p> </li> <li> <p>raster.eoapi.dev/searches/2f3073257a5b6530aedbb0e4b4f726fa/tilejson.json?assets=visual&amp;minzoom=12&amp;maxzoom=19 get a TileJSON document for the <code>2f3073257a5b6530aedbb0e4b4f726fa</code> mosaic and using the <code>visual</code> Asset</p> </li> </ul>"},{"location":"services/#vector","title":"Vector","text":"<p>The OGC Features and (Mapbox Vector) Tiles API service deployed in <code>eoAPI</code> is built on top of tipg).</p> <p>It enables vector Features/Features Collection exploration and visualization for Tables stored in the Postgres database (in the <code>public</code> schema).</p> <p> </p> <p>Example</p> <ul> <li>vector.eoapi.dev landing page</li> </ul> <p>OGC Features</p> <ul> <li> <p>vector.eoapi.dev/collections list available Tables or Function Layers</p> </li> <li> <p>vector.eoapi.dev/collections/public.countries get information about the <code>countries</code> Table</p> </li> <li> <p>vector.eoapi.dev/collections/public.countries/items list items for the <code>countries</code> Table</p> </li> </ul> <p>OGC Tiles</p> <ul> <li> <p>vector.eoapi.dev/collections/public.countries/tiles list all TileSet available for the <code>countries</code> Table</p> </li> <li> <p>vector.eoapi.dev/collections/public.countries/tiles/WebMercatorQuad get <code>WebMercatorQuad</code> TileSet information for the <code>countries</code> Table</p> </li> <li> <p>vector.eoapi.dev/collections/public.countries/viewer shows the <code>countries</code> Table on a Map client using vector tiles</p> </li> <li> <p>vector.eoapi.dev/tileMatrixSets/WebMercatorQuad <code>WebMercatorQuad</code> TileMatrixSet information</p> </li> </ul>"},{"location":"services/#browsing-ui","title":"Browsing UI","text":"<p>The browsing UI deployed in eoAPI is built on the radiant earth STAC browser, and provides a configurable, user-friendly interface to search across and within collections and quickly visualize single items assets.</p> <p> </p> <p>Example</p> <ul> <li>eoapi-dev-stac-browser.s3-website-us-east-1.amazonaws.com landing page</li> </ul>"},{"location":"deployment/aws/","title":"AWS CDK Deployment","text":"<p>\u2190 Back to Deployment</p> <p>Serverless deployment of eoAPI on AWS using Lambda, RDS, and API Gateway.</p>"},{"location":"deployment/aws/#architecture","title":"Architecture","text":"<p>eoAPI on AWS CDK provides: - AWS Lambda functions for API services - Amazon RDS PostgreSQL database - API Gateway for routing and management - CloudFormation for infrastructure as code - Automatic scaling based on request volume - Pay-per-request pricing model</p>"},{"location":"deployment/aws/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS account with appropriate permissions</li> <li>AWS CLI configured</li> <li>Node.js 14+</li> <li>Python 3.8+</li> <li>AWS CDK CLI</li> </ul>"},{"location":"deployment/aws/#quick-start","title":"Quick Start","text":"<ol> <li> <p>Clone and Setup <pre><code>git clone https://github.com/developmentseed/eoapi-template.git\ncd eoapi-template\n\npython -m venv .venv\nsource .venv/bin/activate\npython -m pip install -r requirements.txt\nnpm install\n</code></pre></p> </li> <li> <p>Configure <pre><code># Copy and edit configuration\ncp config.yaml.example config.yaml\n# Edit config.yaml with your settings\n</code></pre></p> </li> <li> <p>Bootstrap CDK (one-time per AWS account) <pre><code>PROJECT_ID=eoAPI STAGE=staging npx cdk bootstrap\n</code></pre></p> </li> <li> <p>Deploy <pre><code>PROJECT_ID=eoAPI STAGE=staging npx cdk deploy vpceoAPI-staging eoAPI-staging\n</code></pre></p> </li> </ol>"},{"location":"deployment/aws/#configuration","title":"Configuration","text":"<p>The deployment is configured through <code>config.yaml</code> and environment variables:</p> <ul> <li><code>PROJECT_ID</code>: Prefix for stack names</li> <li><code>STAGE</code>: Environment name (staging, production, etc.)</li> <li>Database settings, Lambda configuration, and API Gateway options</li> </ul> <p>See configuration examples for detailed options.</p>"},{"location":"deployment/aws/#components","title":"Components","text":"<ul> <li>eoapi-cdk - CDK constructs library</li> <li>eoapi-template - Example CDK application</li> </ul>"},{"location":"deployment/aws/#troubleshooting","title":"Troubleshooting","text":"<p>VPC Limits: If you encounter \"max VPCs reached\" errors, switch to a different AWS region.</p> <p>Permissions: Ensure your AWS credentials have permissions for Lambda, RDS, API Gateway, and CloudFormation.</p>"},{"location":"deployment/kubernetes/","title":"eoAPI Kubernetes","text":"<p>Production-ready Kubernetes deployment for eoAPI.</p> <p>The source code is maintained in the eoapi-k8s repository. Contributions are welcome!</p>"},{"location":"deployment/kubernetes/#kubernetes-architecture","title":"Kubernetes Architecture","text":"<p>This deployment provides:</p> <ul> <li>Path-based ingress routing (<code>/stac</code>, <code>/raster</code>, <code>/vector</code>, <code>/browser</code>, ..)</li> <li>A PostgreSQL cluster (via PostgreSQL Operator)</li> <li>TLS termination and certificate management</li> <li>Persistent storage with dynamic volume provisioning</li> <li>Horizontal pod autoscaling with custom metrics</li> <li>Built-in health checks and monitoring at <code>/stac/_mgmt/ping</code>, <code>/raster/healthz</code>, <code>/vector/healthz</code></li> </ul>"},{"location":"deployment/kubernetes/#getting-started","title":"Getting Started","text":"<p>Ready to deploy? Start with our Quick Start guide for fast installation, or explore the full documentation below for production deployments.</p> <p>Documentation last synchronized: 2025-12-06 06:03:56 UTC</p>"},{"location":"deployment/kubernetes/autoscaling/","title":"Autoscaling","text":"<p>Horizontal Pod Autoscaler (HPA) configuration for eoAPI services. Autoscaling requires monitoring components to be enabled in the main chart.</p>"},{"location":"deployment/kubernetes/autoscaling/#prerequisites","title":"Prerequisites","text":"<p>Enable monitoring in your main eoapi installation:</p> <pre><code>monitoring:\n  prometheus:\n    enabled: true\n  prometheusAdapter:\n    enabled: true  # Required for request-rate scaling\n  metricsServer:\n    enabled: true   # Required for CPU scaling\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#configuration","title":"Configuration","text":""},{"location":"deployment/kubernetes/autoscaling/#basic-autoscaling","title":"Basic Autoscaling","text":"<p>The following instructions assume you've gone through the AWS or GCP cluster set up and installed the <code>eoapi</code> chart.</p> <pre><code>stac:\n  autoscaling:\n    enabled: true\n    minReplicas: 2\n    maxReplicas: 20\n    type: \"requestRate\"  # Options: \"cpu\", \"requestRate\", \"both\"\n    targets:\n      requestRate: 50000m  # 50 requests/second\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#concurrency-settings","title":"Concurrency settings","text":"<p>Each main eoAPI service has <code>WEB_CONCURRENCY</code> and database pool settings that should be adjusted based on your scaling strategy:</p>"},{"location":"deployment/kubernetes/autoscaling/#without-autoscaling-default","title":"Without autoscaling (default)","text":"<p>Higher concurrency per pod to handle some considerate load:</p> <pre><code>stac:\n  settings:\n    envVars:\n      WEB_CONCURRENCY: \"10\"  # More workers per pod\n      DB_MIN_CONN_SIZE: \"1\"\n      DB_MAX_CONN_SIZE: \"5\"  # Total: 10-50 connections per pod\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#with-autoscaling-enabled","title":"With autoscaling enabled","text":"<p>Lower concurrency for predictable resource usage:</p> <pre><code>stac:\n  autoscaling:\n    enabled: true\n  settings:\n    envVars:\n      WEB_CONCURRENCY: \"4\"   # Fewer workers, let HPA scale pods\n      DB_MIN_CONN_SIZE: \"1\"\n      DB_MAX_CONN_SIZE: \"3\"  # Total: 4-12 connections per pod\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#service-specific-recommentations","title":"Service-specific recommentations","text":"Service WEB_CONCURRENCY (no autoscaling) WEB_CONCURRENCY (with autoscaling) Rationale STAC 10 4-6 High request volume, DB intensive Raster 4 2-3 CPU intensive image operations Vector 8 4-5 Complex spatial queries"},{"location":"deployment/kubernetes/autoscaling/#scaling-policies","title":"Scaling Policies","text":"<ol> <li>Go to the releases section of this repository and find the latest <code>eoapi-support-&lt;version&gt;</code> version to install, or use the following command to get the latest version:</li> </ol> <pre><code># Get latest eoapi-support chart version\nexport SUPPORT_VERSION=$(helm search repo eoapi/eoapi-support --versions | head -2 | tail -1 | awk '{print $2}')\n</code></pre> <pre><code>stac:\n  autoscaling:\n    enabled: true\n    minReplicas: 2\n    maxReplicas: 20\n    type: \"both\"\n    behavior:\n      scaleDown:\n        stabilizationWindowSeconds: 300  # 5min cooldown\n        policies:\n        - type: Percent\n          value: 50      # Max 50% pods removed per period\n          periodSeconds: 300\n      scaleUp:\n        stabilizationWindowSeconds: 60   # 1min cooldown\n        policies:\n        - type: Percent\n          value: 100     # Max 100% pods added per period\n          periodSeconds: 60\n    targets:\n      cpu: 70\n      requestRate: 50000m\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#metrics-types","title":"Metrics Types","text":""},{"location":"deployment/kubernetes/autoscaling/#cpu-based-scaling","title":"CPU-based Scaling","text":"<pre><code>type: \"cpu\"\ntargets:\n  cpu: 70\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#request-rate-scaling","title":"Request Rate Scaling","text":"<pre><code>type: \"requestRate\"\ntargets:\n  requestRate: 50000m  # 50 requests/second\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#combined-scaling","title":"Combined Scaling","text":"<pre><code>type: \"both\"\ntargets:\n  cpu: 70\n  requestRate: 100000m  # 100 requests/second\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#custom-metrics-configuration","title":"Custom Metrics Configuration","text":"<p>When using request rate scaling, the prometheus-adapter needs to be configured to expose custom metrics. This is handled automatically when you enable monitoring in the main chart:</p> <pre><code># In your main eoapi values file\ningress:\n  host: your-domain.com\n\nmonitoring:\n  prometheusAdapter:\n    enabled: true\n    resources:\n      limits:\n        cpu: 250m\n        memory: 256Mi\n      requests:\n        cpu: 100m\n        memory: 128Mi\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#service-specific-examples","title":"Service-Specific Examples","text":""},{"location":"deployment/kubernetes/autoscaling/#stac-high-throughput","title":"STAC (High throughput)","text":"<pre><code>stac:\n  autoscaling:\n    enabled: true\n    minReplicas: 3\n    maxReplicas: 20\n    type: \"requestRate\"\n    targets:\n      requestRate: 40000m\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#raster-resource-intensive","title":"Raster (Resource intensive)","text":"<pre><code>raster:\n  autoscaling:\n    enabled: true\n    minReplicas: 2\n    maxReplicas: 8\n    type: \"cpu\"\n    behavior:\n      scaleDown:\n        stabilizationWindowSeconds: 300\n    targets:\n      cpu: 75\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#vector-balanced","title":"Vector (Balanced)","text":"<pre><code>vector:\n  autoscaling:\n    enabled: true\n    minReplicas: 2\n    maxReplicas: 12\n    type: \"both\"\n    targets:\n      cpu: 70\n      requestRate: 75000m\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#configuration-examples","title":"Configuration Examples","text":"<p>For complete configuration examples, see the production profile.</p>"},{"location":"deployment/kubernetes/autoscaling/#resource-requirements","title":"Resource Requirements","text":""},{"location":"deployment/kubernetes/autoscaling/#autoscaling-components","title":"Autoscaling Components","text":"<ul> <li>metrics-server: ~100m CPU, ~300Mi memory per node</li> <li>prometheus-adapter: ~250m CPU, ~256Mi memory</li> <li>prometheus-server: ~500m CPU, ~512Mi memory (varies with retention)</li> </ul>"},{"location":"deployment/kubernetes/autoscaling/#verification","title":"Verification","text":""},{"location":"deployment/kubernetes/autoscaling/#check-hpa-status","title":"Check HPA Status","text":"<pre><code># Check HPA status for all services\nkubectl get hpa -n eoapi\n\n# Get detailed HPA information\nkubectl describe hpa eoapi-stac -n eoapi\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#verify-custom-metrics-api","title":"Verify Custom Metrics API","text":"<pre><code># Check if custom metrics API is available\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1\" | jq .\n\n# Check specific request rate metrics\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/namespaces/eoapi/ingresses/*/requests_per_second\" | jq .\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#check-prometheus-adapter","title":"Check Prometheus Adapter","text":"<pre><code># Check prometheus-adapter logs\nkubectl logs -l app.kubernetes.io/name=prometheus-adapter -n eoapi\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#load-testing","title":"Load Testing","text":"<p>For load testing your autoscaling setup:</p> <pre><code>ingress:\n  host: your-test-domain.com\n</code></pre> <ol> <li>Check ingress configuration:    <pre><code>kubectl get ingress -n eoapi\n</code></pre></li> </ol>"},{"location":"deployment/kubernetes/autoscaling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/kubernetes/autoscaling/#hpa-shows-unknown-metrics","title":"HPA Shows \"Unknown\" Metrics","text":"<p>If HPA shows \"unknown\" for custom metrics:</p> <ol> <li> <p>Verify prometheus-adapter is running:    <pre><code>kubectl get pods -l app.kubernetes.io/name=prometheus-adapter -n eoapi\n</code></pre></p> </li> <li> <p>Check prometheus-adapter logs:    <pre><code>kubectl logs -l app.kubernetes.io/name=prometheus-adapter -n eoapi\n</code></pre></p> </li> <li> <p>Verify metrics are available in Prometheus:    <pre><code># Port forward to access Prometheus\nkubectl port-forward service/eoapi-prometheus-server 9090:80 -n eoapi\n# Then check metrics at http://localhost:9090\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/autoscaling/#default-configuration","title":"Default Configuration","text":"<p>Default autoscaling configuration:</p> <pre><code>autoscaling:\n  enabled: false\n  minReplicas: 1\n  maxReplicas: 5\n  # Type can be \"cpu\", \"requestRate\", or \"both\"\n  type: \"cpu\"\n  # Custom scaling behavior (optional)\n  behavior: {}\n  # Scaling targets\n  targets:\n    # CPU target percentage (when type is \"cpu\" or \"both\")\n    cpu: 80\n    # Request rate target in millirequests per second (when type is \"requestRate\" or \"both\")\n    requestRate: 30000m\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#no-scaling-activity","title":"No Scaling Activity","text":"<p>If pods aren't scaling:</p> <ol> <li> <p>Check HPA events:    <pre><code>kubectl describe hpa eoapi-stac -n eoapi\n</code></pre></p> </li> <li> <p>Verify metrics are being collected:    <pre><code>kubectl top pods -n eoapi\n</code></pre></p> </li> <li> <p>Check resource requests are set:    <pre><code>kubectl describe pod eoapi-stac-xxx -n eoapi | grep -A 10 \"Requests\"\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/autoscaling/#install-or-upgrade-autoscaling-changes-to-eoapi-chart","title":"Install or Upgrade Autoscaling Changes to <code>eoapi</code> Chart","text":"<p>When enabling autoscaling, ensure monitoring is also enabled:</p> <pre><code># Enable monitoring first\nmonitoring:\n  prometheus:\n    enabled: true\n  prometheusAdapter:\n    enabled: true\n\n# Then enable autoscaling\nstac:\n  autoscaling:\n    enabled: true\n    type: \"requestRate\"\n    targets:\n      requestRate: 50000m\n\n# Configure resources for proper scaling metrics\nstac:\n  settings:\n    resources:\n      limits:\n        cpu: 1000m\n        memory: 512Mi\n      requests:\n        cpu: 100m\n        memory: 128Mi\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#custom-metrics-not-working","title":"Custom Metrics Not Working","text":"<p>If request rate metrics aren't working:</p> <ol> <li>Verify nginx ingress controller has metrics enabled</li> <li>Check prometheus is scraping ingress metrics</li> <li>Confirm prometheus-adapter configuration</li> <li>Validate ingress annotations for metrics</li> </ol>"},{"location":"deployment/kubernetes/autoscaling/#scaling-too-aggressiveslow","title":"Scaling Too Aggressive/Slow","text":"<p>Adjust scaling behavior:</p> <pre><code>autoscaling:\n  behavior:\n    scaleUp:\n      stabilizationWindowSeconds: 60    # Faster scaling up\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 60\n    scaleDown:\n      stabilizationWindowSeconds: 300   # Slower scaling down\n      policies:\n      - type: Percent\n        value: 25                       # More conservative scale down\n        periodSeconds: 300\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#best-practices","title":"Best Practices","text":"<ol> <li>Set appropriate resource requests: HPA needs resource requests to calculate CPU utilization</li> <li>Use stabilization windows: Prevent thrashing with appropriate cooldown periods</li> <li>Monitor costs: Autoscaling can increase costs rapidly</li> <li>Test thoroughly: Validate scaling behavior under realistic load</li> <li>Set reasonable limits: Use <code>maxReplicas</code> to prevent runaway scaling</li> <li>Use multiple metrics: Combine CPU and request rate for better scaling decisions</li> </ol> <p>Example ingress configuration for load testing:</p> <pre><code># For AWS ALB\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: eoapi-ingress\n  annotations:\n    kubernetes.io/ingress.class: alb\n    alb.ingress.kubernetes.io/scheme: internet-facing\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: your-domain.com\n    http:\n      paths: [...]\n\n# For nginx ingress\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: eoapi-ingress\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: abc5929f88f8c45c38f6cbab2faad43c-776419634.us-west-2.elb.amazonaws.com\n    http:\n      paths: [...]\n</code></pre>"},{"location":"deployment/kubernetes/autoscaling/#load-testing_1","title":"Load Testing","text":""},{"location":"deployment/kubernetes/autoscaling/#load-testing-with-hey","title":"Load Testing with <code>hey</code>","text":"<p>The <code>hey</code> tool is a simple HTTP load testing tool.</p>"},{"location":"deployment/kubernetes/autoscaling/#install-and-run-load-tests","title":"Install and Run Load Tests","text":"<ol> <li> <p>Install hey:    <pre><code># macOS\nbrew install hey\n\n# Linux\ngo install github.com/rakyll/hey@latest\n\n# Or download from releases\nwget https://hey-release.s3.us-east-2.amazonaws.com/hey_linux_amd64\nchmod +x hey_linux_amd64\nsudo mv hey_linux_amd64 /usr/local/bin/hey\n</code></pre></p> </li> <li> <p>Run basic load test:    <pre><code># Test STAC endpoint\nhey -z 5m -c 10 https://your-domain.com/stac/collections\n\n# Test with higher concurrency\nhey -z 10m -c 50 https://your-domain.com/stac/search\n</code></pre></p> </li> <li> <p>Monitor during load test:    <pre><code># Watch HPA scaling\nwatch kubectl get hpa -n eoapi\n\n# Monitor pods\nwatch kubectl get pods -n eoapi\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/autoscaling/#load-testing-best-practices","title":"Load Testing Best Practices","text":"<ol> <li>Start small: Begin with low concurrency and short duration</li> <li>Monitor resources: Watch CPU, memory, and network usage</li> <li>Test realistic scenarios: Use actual API endpoints and payloads</li> <li>Gradual increase: Slowly increase load to find breaking points</li> <li>Test different endpoints: Each service may have different characteristics</li> </ol>"},{"location":"deployment/kubernetes/autoscaling/#troubleshooting-load-tests","title":"Troubleshooting Load Tests","text":"<ul> <li>High response times: May indicate need for more replicas or resources</li> <li>Error rates: Could suggest database bottlenecks or resource limits</li> <li>No scaling: Check HPA metrics and thresholds</li> </ul>"},{"location":"deployment/kubernetes/autoscaling/#advanced-load-testing","title":"Advanced Load Testing","text":"<p>For more comprehensive testing, consider: - Artillery - Feature-rich load testing toolkit - k6 - Developer-centric performance testing - Locust - Python-based distributed load testing</p> <p>For monitoring and observability setup, see observability.md.</p>"},{"location":"deployment/kubernetes/aws-eks/","title":"AWS EKS Cluster Walkthrough","text":"<p>This is a verbose walkthrough. It uses <code>eksctl</code> and assumes you already have an AWS account, have the eksctl prerequisites installed including <code>eksctl</code> and <code>helm</code>.</p> <p>If you're familiar with Terraform and would like an IaC choice that is more terse consider setting up your cluster with that: developmentseed/eoapi-k8s-terraform</p>"},{"location":"deployment/kubernetes/aws-eks/#table-of-contents","title":"Table of Contents:","text":"<ol> <li>Create EKS Cluster</li> <li>Make sure EKS Cluster has OIDC Provider</li> <li>Install Node Autoscaling</li> <li>Install EBS CSI Add-on</li> <li>Install NGINX Ingress Controller</li> </ol>"},{"location":"deployment/kubernetes/aws-eks/#create-your-k8s-cluster","title":"Create your k8s cluster","text":"<p>An example command below. See the eksctl docs for all the options</p> <pre><code># Useful ssh-access if you want to ssh into your nodes\n# Check latest supported versions: eksctl get clusters --region us-west-2\neksctl create cluster \\\n    --name sandbox \\\n    --region us-west-2 \\\n    --ssh-access=true \\\n    --ssh-public-key=~/.ssh/id_ed25519_k8_sandbox.pub \\\n    --nodegroup-name=hub-node \\\n    --node-type=t2.medium \\\n    --nodes=1 --nodes-min=1 --nodes-max=5 \\\n    --version 1.28 \\\n    --asg-access\n</code></pre> <p>TODO:  Add autoscaling config</p> <p>Note: To generate your <code>ssh-public-key</code>, use:</p> <pre><code>ssh-keygen -t ed25519 -f ~/.ssh/id_ed25519_k8_sandbox\n</code></pre> <p>You might need to iterate on the command above, so to delete the cluster:</p> <pre><code>eksctl delete cluster --name=sandbox --region us-west-2\n</code></pre>"},{"location":"deployment/kubernetes/aws-eks/#check-oidc-provider-set-up-for-you-cluster","title":"Check OIDC provider set up for you cluster","text":"<p>For k8s <code>ServiceAccount</code>(s) to do things on behalf of pods in AWS you need an OIDC provider set up. Best to walk through the AWS docs for this but below are the relevant bits. Note that <code>eksctl</code> \"should\" set up an OIDC provider for you by default</p> <pre><code>export CLUSTER_NAME=sandbox\nexport REGION=us-west-2\n\noidc_id=$(aws eks describe-cluster --name $CLUSTER_NAME --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5)\nexisting_oidc_id=$(aws iam list-open-id-connect-providers | grep $oidc_id | cut -d \"/\" -f4)\nif [ -z \"$existing_oidc_id\" ]; then\n echo \"no existing OIDC provider, associating one...\"\n eksctl utils associate-iam-oidc-provider --cluster $CLUSTER_NAME --region $REGION --approve\nelse\n echo \"already have an existing OIDC provider, skipping...\"\nfi\n</code></pre>"},{"location":"deployment/kubernetes/aws-eks/#install-node-autoscaler","title":"Install Node Autoscaler","text":"<p>To run the autoscaler on EKS, we need to create a policy and install Auto-Discovery Setup, which is the preferred method to configure Cluster Autoscaler in EKS.</p> <pre><code>aws iam create-policy --policy-name ${CLUSTER_NAME}-asg --policy-document file://configs/aws-asg-policy.json\n# Update the cluster name ins case you changed in configs/cluster-autoscaler-autodiscover.yaml file\nkubectl apply -f configs/cluster-autoscaler-autodiscover.yaml\n</code></pre>"},{"location":"deployment/kubernetes/aws-eks/#install-the-ebs-csi-addon-for-dynamic-ebs-provisioning","title":"Install the EBS CSI Addon for dynamic EBS provisioning","text":"<p>Best to walk through the AWS docs about this since there are potential version conflicts and footguns</p> <p>First, create an IAM Role for the future EBS CSI <code>ServiceAccount</code> binding:</p> <p>\u24d8 the AWS docs make it seem like the k8 <code>ServiceAccount</code> and related <code>kind: Controller</code> are already created, but they aren't</p> <pre><code>eksctl create iamserviceaccount \\\n    --region us-west-2 \\\n    --name ebs-csi-controller-sa \\\n    --namespace kube-system \\\n    --cluster sandbox \\\n    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \\\n    --approve \\\n    --role-only \\\n    --role-name eksctl-veda-sandbox-addon-aws-ebs-csi-driver # arbitrary, the naming is up to you\n</code></pre> <p>Check the compatible EBS CSI addon version for your cluster version using AWS docs.</p> <pre><code># Get your cluster version\nexport CLUSTER_VERSION=$(aws eks describe-cluster --name $CLUSTER_NAME --region $REGION --query \"cluster.version\" --output text)\n\n# Find compatible addon versions\naws eks describe-addon-versions \\\n    --addon-name aws-ebs-csi-driver \\\n    --kubernetes-version $CLUSTER_VERSION \\\n    --region $REGION \\\n    --query \"addons[0].addonVersions[0].addonVersion\" --output text\n</code></pre> <p>Then create the EBS CSI Addon:</p> <p>\u24d8 note that this step creates k8 <code>ServiceAccount</code> and ebs-csi pods and <code>kind: Controller</code></p> <pre><code># Get the latest compatible addon version and install\nexport AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity --query Account --output text)\"\nexport CLUSTER_VERSION=$(aws eks describe-cluster --name $CLUSTER_NAME --region $REGION --query \"cluster.version\" --output text)\nexport EBS_CSI_VERSION=$(aws eks describe-addon-versions \\\n    --addon-name aws-ebs-csi-driver \\\n    --kubernetes-version $CLUSTER_VERSION \\\n    --region $REGION \\\n    --query \"addons[0].addonVersions[0].addonVersion\" --output text)\n\neksctl create addon \\\n    --name aws-ebs-csi-driver \\\n    --region $REGION \\\n    --cluster $CLUSTER_NAME \\\n    --version $EBS_CSI_VERSION \\\n    --service-account-role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/eksctl-veda-sandbox-addon-aws-ebs-csi-driver \\\n    --force\n</code></pre> <p>Finally, do some checking to assert things are set up correctly:</p> <pre><code># Check to make the ServiceAccount has an annotation of your IAM role\nkubectl get sa ebs-csi-controller-sa -n kube-system -o yaml | grep -a1 annotations\n# metadata:\n#     annotations:\n#         eks.amazonaws.com/role-arn: arn:aws:iam::&lt;AWS_ACCOUNT_ID&gt;:role/eksctl-veda-sandbox-addon-aws-ebs-csi-driver\n</code></pre> <pre><code># Check to make sure we have controller pods up for ebs-csi and that they aren't in state `CrashLoopBack`\nkubectl get pod  -n kube-system | grep ebs-csi\n# ebs-csi-controller-5cbc775dc5-hr6mz   6/6     Running   0          4m51s\n# ebs-csi-controller-5cbc775dc5-knqnr   6/6     Running   0          4m51s\n</code></pre> <p>You can additionally run through these AWS docs to deploy a sample application to make sure it dynamically mounts an EBS volume</p>"},{"location":"deployment/kubernetes/aws-eks/#install-nginx-ingress-controller","title":"Install Nginx Ingress Controller","text":"<p>Please look through the Nginx Docs to verify nothing has changed below. There are multiple ways to provision and configure. Below is the simplest we found:</p> <pre><code>helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\nkubectl create namespace ingress-nginx\nhelm upgrade -i ingress-nginx \\\n    ingress-nginx/ingress-nginx \\\n    --set controller.service.type=LoadBalancer \\\n    --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-type\"=\"nlb\" \\\n    --set controller.service.annotations.\"service\\.beta\\.kubernetes\\.io/aws-load-balancer-scheme\"=\"internet-facing\" \\\n    --namespace ingress-nginx\n\n# helm delete ingress-nginx -n ingress-nginx\n</code></pre> <p>Depending on what NGINX functionality you need you might also want to configure <code>kind: ConfigMap</code> as talked about on their docs. Below we enable gzip by patching <code>use-gzip</code> into the <code>ConfigMap</code>:</p> <pre><code>kubectl get cm  | grep ingress-nginx | cut -d' ' -f1 | xargs -I{} kubectl patch cm/{} --type merge -p '{\"data\":{\"use-gzip\":\"true\"}}'\n\n# kubectl get cm --all-namespaces| grep ingress-nginx-controller | awk '{print $1 \" \" $2}' | while read ns cm; do kubectl patch cm -n $ns $cm --type merge -p '{\"data\":{\"use-gzip\":\"true\"}}'; done\n\nkubectl get deploy --all-namespaces | grep ingress-nginx-conto | cut -d' ' -f1 | xargs -I{} kubectl rollout restart deploy/{}\n\n# kubectl get deploy --all-namespaces| grep ingress-nginx-controller | awk '{print $1 \" \" $2}' | while read ns deploy; do kubectl rollout restart deploy/$deploy -n $ns; done\n</code></pre> <p>Assert that things are set up correctly:</p> <pre><code>kubectl get deploy,pod,svc --all-namespaces | grep nginx\n\n# deployment.apps/nginx-ingress-nginx-controller   1/1     1            1           2d17h\n# pod/nginx-ingress-nginx-controller-76d7f6f4d5-g6fkv   1/1     Running   0          27h\n# service/nginx-ingress-nginx-controller             LoadBalancer   10.100.36.152    eoapi-k8s-553d3ea234b-3eef2e6e61e5d161.elb.us-west-1.amazonaws.com   80:30342/TCP,443:30742/TCP   2d17h\n# service/nginx-ingress-nginx-controller-admission   ClusterIP      10.100.34.22     &lt;none&gt;                                                                          443/TCP                      2d17h\n</code></pre>"},{"location":"deployment/kubernetes/aws-eks/#configure-irsa-for-s3-bucket-access","title":"Configure IRSA for S3 Bucket Access","text":"<p>eoAPI services (especially the raster API) need to access COG files in S3 buckets. Instead of using long-lived credentials, use IRSA (IAM Roles for Service Accounts) for secure, temporary credential access:</p> <ol> <li>Create an IAM policy for S3 bucket access:</li> </ol> <pre><code>cat &lt;&lt;EOF &gt; eoapi-s3-policy.json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:GetObject\",\n        \"s3:ListBucket\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::your-bucket-name/*\",\n        \"arn:aws:s3:::your-bucket-name\"\n      ]\n    }\n  ]\n}\nEOF\n\naws iam create-policy \\\n    --policy-name eoapi-s3-access \\\n    --policy-document file://eoapi-s3-policy.json\n</code></pre> <ol> <li>Create an IAM role and service account:</li> </ol> <pre><code>eksctl create iamserviceaccount \\\n    --name eoapi-sa \\\n    --namespace default \\\n    --cluster $CLUSTER_NAME \\\n    --region $REGION \\\n    --attach-policy-arn arn:aws:iam::${AWS_ACCOUNT_ID}:policy/eoapi-s3-access \\\n    --approve\n</code></pre> <p>This command automatically creates a service account with the necessary annotation:    <pre><code># This is what eksctl creates for you:\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: eoapi-sa\n  namespace: default\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/eksctl-${CLUSTER_NAME}-addon-iamserviceaccount-default-eoapi-sa\n</code></pre></p> <ol> <li>Configure EOAPI to use the service account in your <code>values.yaml</code>:</li> </ol> <pre><code>serviceAccount:\n  create: false  # We already created it with eksctl\n  name: eoapi-sa\n</code></pre> <p>Alternative: If you prefer to let the Helm chart create the service account, skip step 2 and use:    <pre><code>serviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::${AWS_ACCOUNT_ID}:role/your-existing-iam-role\n</code></pre></p> <p>The raster service will automatically use IRSA credentials via the AWS SDK credential chain. No hardcoded credentials needed!</p>"},{"location":"deployment/kubernetes/azure/","title":"Microsoft Azure Setup","text":""},{"location":"deployment/kubernetes/azure/#using-azure-managed-postgresql","title":"Using Azure Managed PostgreSQL","text":"<p>With the unified PostgreSQL configuration, connecting to an Azure managed PostgreSQL instance has become more straightforward. Here's how to set it up:</p> <ol> <li>Create an Azure PostgreSQL server: Create a PostgreSQL server using the Azure portal or the Azure CLI.</li> </ol> <pre><code># Example of creating an Azure PostgreSQL flexible server\naz postgres flexible-server create \\\n  --resource-group myResourceGroup \\\n  --name mypostgresserver \\\n  --location westus \\\n  --admin-user myusername \\\n  --admin-password mypassword \\\n  --sku-name Standard_B1ms\n</code></pre> <ol> <li>Create a PostgreSQL database: After creating the server, create a database for your EOAPI deployment.</li> </ol> <pre><code># Create a database on the Azure PostgreSQL server\naz postgres flexible-server db create \\\n  --resource-group myResourceGroup \\\n  --server-name mypostgresserver \\\n  --database-name eoapi\n</code></pre> <ol> <li>Configure firewall rules: Ensure that the PostgreSQL server allows connections from your Kubernetes cluster's IP address.</li> </ol> <pre><code># Allow connections from your AKS cluster's outbound IP\naz postgres flexible-server firewall-rule create \\\n  --resource-group myResourceGroup \\\n  --server-name mypostgresserver \\\n  --name AllowAKS \\\n  --start-ip-address &lt;AKS-outbound-IP&gt; \\\n  --end-ip-address &lt;AKS-outbound-IP&gt;\n</code></pre> <ol> <li>Store PostgreSQL credentials in Azure Key Vault: Create secrets in your Azure Key Vault to store the database connection information.</li> </ol> <pre><code># Create Key Vault secrets for PostgreSQL connection\naz keyvault secret set --vault-name your-keyvault-name --name db-host --value \"mypostgresserver.postgres.database.azure.com\"\naz keyvault secret set --vault-name your-keyvault-name --name db-port --value \"5432\"\naz keyvault secret set --vault-name your-keyvault-name --name db-name --value \"eoapi\"\naz keyvault secret set --vault-name your-keyvault-name --name db-username --value \"myusername@mypostgresserver\"\naz keyvault secret set --vault-name your-keyvault-name --name db-password --value \"mypassword\"\n</code></pre>"},{"location":"deployment/kubernetes/azure/#azure-configuration-for-eoapi-k8s","title":"Azure Configuration for eoapi-k8s","text":"<p>When deploying on Azure, you'll need to configure several settings in your values.yaml file. Below are the configurations needed for proper integration with Azure services.</p>"},{"location":"deployment/kubernetes/azure/#common-azure-configuration","title":"Common Azure Configuration","text":"<p>First, configure the service account with Azure Workload Identity:</p> <pre><code># Service Account Configuration\nserviceAccount:\n  create: true\n  annotations:\n    azure.workload.identity/client-id: \"your-client-id\"\n    azure.workload.identity/tenant-id: \"your-tenant-id\"\n</code></pre>"},{"location":"deployment/kubernetes/azure/#unified-postgresql-configuration","title":"Unified PostgreSQL Configuration","text":"<p>Use the unified PostgreSQL configuration with the <code>external-secret</code> type to connect to your Azure managed PostgreSQL:</p> <pre><code># Configure PostgreSQL connection to use Azure managed PostgreSQL with secrets from Key Vault\npostgresql:\n  # Use external-secret type to get credentials from a pre-existing secret\n  type: \"external-secret\"\n\n  # Basic connection information\n  external:\n    host: \"mypostgresserver.postgres.database.azure.com\"  # Can be overridden by secret values\n    port: \"5432\"                                          # Can be overridden by secret values\n    database: \"eoapi\"                                     # Can be overridden by secret values\n\n    # Reference to a secret that will be created by Azure Key Vault integration\n    existingSecret:\n      name: \"azure-pg-credentials\"\n      keys:\n        username: \"username\"     # Secret key for the username\n        password: \"password\"     # Secret key for the password\n        host: \"host\"             # Secret key for the host (optional)\n        port: \"port\"             # Secret key for the port (optional)\n        database: \"database\"     # Secret key for the database name (optional)\n</code></pre> <p>With this configuration, you're telling the PostgreSQL components to use an external PostgreSQL database and to get its connection details from a Kubernetes secret named <code>azure-pg-credentials</code>. This secret will be created using Azure Key Vault integration as described below.</p>"},{"location":"deployment/kubernetes/azure/#disable-internal-postgresql-cluster","title":"Disable internal PostgreSQL cluster","text":"<p>When using Azure managed PostgreSQL, you should disable the internal PostgreSQL cluster:</p> <pre><code>postgrescluster:\n  enabled: false\n</code></pre>"},{"location":"deployment/kubernetes/azure/#azure-key-vault-integration","title":"Azure Key Vault Integration","text":"<p>To allow your Kubernetes pods to access PostgreSQL credentials stored in Azure Key Vault, create a SecretProviderClass:</p> <pre><code>apiVersion: secrets-store.csi.x-k8s.io/v1\nkind: SecretProviderClass\nmetadata:\n  name: azure-pg-secret-provider\nspec:\n  provider: azure\n  parameters:\n    usePodIdentity: \"false\"\n    clientID: \"your-client-id\"\n    keyvaultName: \"your-keyvault-name\"\n    tenantId: \"your-tenant-id\"\n    objects: |\n      array:\n        - |\n          objectName: db-host\n          objectType: secret\n          objectAlias: host\n        - |\n          objectName: db-port\n          objectType: secret\n          objectAlias: port\n        - |\n          objectName: db-name\n          objectType: secret\n          objectAlias: database\n        - |\n          objectName: db-username\n          objectType: secret\n          objectAlias: username\n        - |\n          objectName: db-password\n          objectType: secret\n          objectAlias: password\n  secretObjects:\n    - secretName: azure-pg-credentials\n      type: Opaque\n      data:\n        - objectName: host\n          key: host\n        - objectName: port\n          key: port\n        - objectName: database\n          key: database\n        - objectName: username\n          key: username\n        - objectName: password\n          key: password\n</code></pre>"},{"location":"deployment/kubernetes/azure/#service-configuration","title":"Service Configuration","text":"<p>For services that need to mount the Key Vault secrets, add the following configuration to each service (pgstacBootstrap, raster, stac, vector, multidim):</p> <pre><code># Define a common volume configuration for all services\ncommonVolumeConfig: &amp;commonVolumeConfig\n  labels:\n    azure.workload.identity/use: \"true\"\n  extraVolumeMounts:\n    - name: azure-keyvault-secrets\n      mountPath: /mnt/secrets-store\n      readOnly: true\n  extraVolumes:\n    - name: azure-keyvault-secrets\n      csi:\n        driver: secrets-store.csi.k8s.io\n        readOnly: true\n        volumeAttributes:\n          secretProviderClass: azure-pg-secret-provider\n\n# Apply the common volume configuration to each service\npgstacBootstrap:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nraster:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nstac:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nvector:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nmultidim:\n  enabled: false  # set to true if needed\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n</code></pre>"},{"location":"deployment/kubernetes/azure/#azure-blob-storage-authentication","title":"Azure Blob Storage Authentication","text":"<p>eoAPI services (particularly the raster API) need to access COG files stored in Azure Blob Storage. With Azure Workload Identity configured (as shown above), authentication happens automatically:</p> <ol> <li> <p>Grant storage access to your managed identity:    <pre><code>CLIENT_ID=$(az identity show -g &lt;resource-group&gt; -n eoapi-identity --query clientId -o tsv)\n\naz role assignment create \\\n  --role \"Storage Blob Data Reader\" \\\n  --assignee $CLIENT_ID \\\n  --scope /subscriptions/&lt;subscription-id&gt;/resourceGroups/&lt;resource-group&gt;/providers/Microsoft.Storage/storageAccounts/&lt;storage-account&gt;\n</code></pre></p> </li> <li> <p>Automatic authentication: The raster service (titiler-pgstac) uses GDAL's <code>/vsiaz/</code> driver, which automatically authenticates using:</p> </li> <li>Workload Identity credentials (via service account annotations)</li> <li>Managed Identity (if running on Azure VMs)</li> <li>Environment variables (if set)</li> </ol> <p>No additional configuration or hardcoded credentials needed!</p>"},{"location":"deployment/kubernetes/azure/#azure-managed-identity-setup","title":"Azure Managed Identity Setup","text":"<p>To use Azure Managed Identity with your Kubernetes cluster:</p> <ol> <li> <p>Enable Workload Identity on your AKS cluster:    <pre><code>az aks update -g &lt;resource-group&gt; -n &lt;cluster-name&gt; --enable-workload-identity\n</code></pre></p> </li> <li> <p>Create a Managed Identity:    <pre><code>az identity create -g &lt;resource-group&gt; -n eoapi-identity\n</code></pre></p> </li> <li> <p>Configure Key Vault access:    <pre><code># Get the client ID of the managed identity\nCLIENT_ID=$(az identity show -g &lt;resource-group&gt; -n eoapi-identity --query clientId -o tsv)\n\n# Grant access to Key Vault\naz keyvault set-policy -n &lt;keyvault-name&gt; --secret-permissions get list --spn $CLIENT_ID\n</code></pre></p> </li> <li> <p>Create a federated identity credential to connect the Kubernetes service account to the Azure managed identity:    <pre><code>az identity federated-credential create \\\n  --name eoapi-federated-credential \\\n  --identity-name eoapi-identity \\\n  --resource-group &lt;resource-group&gt; \\\n  --issuer &lt;aks-oidc-issuer&gt; \\\n  --subject system:serviceaccount:&lt;namespace&gt;:eoapi-sa\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/azure/#complete-example","title":"Complete Example","text":"<p>Here's a complete example configuration for connecting EOAPI to an Azure managed PostgreSQL database:</p> <pre><code># Service Account Configuration with Azure Workload Identity\nserviceAccount:\n  create: true\n  annotations:\n    azure.workload.identity/client-id: \"12345678-1234-1234-1234-123456789012\"\n    azure.workload.identity/tenant-id: \"87654321-4321-4321-4321-210987654321\"\n\n# Unified PostgreSQL Configuration - using external-secret type\npostgresql:\n  type: \"external-secret\"\n  external:\n    host: \"mypostgresserver.postgres.database.azure.com\"\n    port: \"5432\"\n    database: \"eoapi\"\n    existingSecret:\n      name: \"azure-pg-credentials\"\n      keys:\n        username: \"username\"\n        password: \"password\"\n        host: \"host\"\n        port: \"port\"\n        database: \"database\"\n\n# Disable internal PostgreSQL cluster\npostgrescluster:\n  enabled: false\n\n# Define common volume configuration with Azure Key Vault integration\ncommonVolumeConfig: &amp;commonVolumeConfig\n  labels:\n    azure.workload.identity/use: \"true\"\n  extraVolumeMounts:\n    - name: azure-keyvault-secrets\n      mountPath: /mnt/secrets-store\n      readOnly: true\n  extraVolumes:\n    - name: azure-keyvault-secrets\n      csi:\n        driver: secrets-store.csi.k8s.io\n        readOnly: true\n        volumeAttributes:\n          secretProviderClass: azure-pg-secret-provider\n\n# Apply the common volume configuration to each service\npgstacBootstrap:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nstac:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nraster:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nvector:\n  enabled: true\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n\nmultidim:\n  enabled: false\n  settings:\n    &lt;&lt;: *commonVolumeConfig\n</code></pre> <p>Make sure to create the SecretProviderClass as shown in the \"Azure Key Vault Integration\" section above before deploying EOAPI with this configuration.</p>"},{"location":"deployment/kubernetes/configuration/","title":"Configuration Options","text":""},{"location":"deployment/kubernetes/configuration/#required-values","title":"Required Values","text":"<p>The required values to pass to <code>helm install</code> or <code>helm template</code> commands can be found in our schema validation:</p> <pre><code>{\n  \"required\": [\n    \"service\",\n    \"gitSha\"\n  ]\n}\n</code></pre> <p>Most fields have sensible defaults. Here are the core configuration options:</p> Values Key Description Default Choices <code>service.port</code> Port for all services (vector/raster/stac) 8080 any valid port <code>gitSha</code> SHA for deployment tracking gitshaABC123 any valid SHA"},{"location":"deployment/kubernetes/configuration/#database-configuration","title":"Database Configuration","text":""},{"location":"deployment/kubernetes/configuration/#postgresql-cluster-default","title":"PostgreSQL Cluster (Default)","text":"<p>Using Crunchydata's PostgreSQL Operator (<code>postgresql.type: \"postgrescluster\"</code>):</p> Values Key Description Default Choices <code>postgrescluster.enabled</code> Enable PostgreSQL cluster. Must be set to <code>false</code> when using external databases true true/false <code>postgrescluster.name</code> Cluster name Release name any valid k8s name <code>postgrescluster.postgresVersion</code> PostgreSQL version 16 supported versions <code>postgrescluster.postGISVersion</code> PostGIS version \"3.4\" supported versions"},{"location":"deployment/kubernetes/configuration/#external-database","title":"External Database","text":"<p>For external databases, set <code>postgresql.type</code> to either <code>external-plaintext</code> or <code>external-secret</code> and set <code>postgrescluster.enabled: false</code>.</p> <ol> <li> <p>Using plaintext credentials (<code>external-plaintext</code>): <pre><code>postgrescluster:\n  enabled: false\npostgresql:\n  type: \"external-plaintext\"\n  external:\n    host: \"your-host\"\n    port: \"5432\"\n    database: \"eoapi\"\n    credentials:\n      username: \"eoapi\"\n      password: \"your-password\"\n</code></pre></p> </li> <li> <p>Using Kubernetes secrets (<code>external-secret</code>): <pre><code>postgresql:\n  type: \"external-secret\"\n  external:\n    existingSecret:\n      name: \"your-secret\"\n      keys:\n        username: \"username\"\n        password: \"password\"\n    host: \"your-host\"  # can also be in secret\n    port: \"5432\"       # can also be in secret\n    database: \"eoapi\"  # can also be in secret\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/configuration/#pgstac-configuration","title":"PgSTAC Configuration","text":"<p>Control PgSTAC database behavior and performance tuning:</p>"},{"location":"deployment/kubernetes/configuration/#core-settings","title":"Core Settings","text":"<p>Configure via <code>pgstacBootstrap.settings.pgstacSettings</code>:</p> Values Key Description Default Format <code>queue_timeout</code> Timeout for queued queries \"10 minutes\" PostgreSQL interval <code>use_queue</code> Enable query queue mechanism \"false\" boolean string <code>update_collection_extent</code> Auto-update collection extents \"true\" boolean string"},{"location":"deployment/kubernetes/configuration/#context-settings","title":"Context Settings","text":"<p>Control search result count calculations:</p> Values Key Description Default Format <code>context</code> Context mode \"auto\" \"on\", \"off\", \"auto\" <code>context_estimated_count</code> Row threshold for estimates \"100000\" integer string <code>context_estimated_cost</code> Cost threshold for estimates \"100000\" integer string <code>context_stats_ttl</code> Stats cache duration \"1 day\" PostgreSQL interval"},{"location":"deployment/kubernetes/configuration/#automatic-maintenance-jobs","title":"Automatic Maintenance Jobs","text":"<p>CronJobs are conditionally created based on PgSTAC settings:</p> <p>Queue Processor (created when <code>use_queue: \"true\"</code>): - <code>queueProcessor.schedule</code>: \"0 * * * *\" (hourly) - Processes queries that exceeded timeout</p> <p>Extent Updater (created when <code>update_collection_extent: \"false\"</code>): - <code>extentUpdater.schedule</code>: \"0 2 * * *\" (daily at 2 AM) - Updates collection spatial/temporal boundaries</p> <p>By default, no CronJobs are created (use_queue=false, update_collection_extent=true).</p> <p>Both schedules are customizable using standard cron format.</p> <p>Example configuration:</p> <pre><code>pgstacBootstrap:\n  settings:\n    pgstacSettings:\n      # Performance tuning for large datasets\n      queue_timeout: \"20 minutes\"\n      use_queue: \"true\"\n      update_collection_extent: \"false\"\n\n      # Optimize context for performance\n      context: \"auto\"\n      context_estimated_count: \"50000\"\n      context_estimated_cost: \"75000\"\n      context_stats_ttl: \"12 hours\"\n</code></pre>"},{"location":"deployment/kubernetes/configuration/#queryables-configuration","title":"Queryables Configuration","text":"<p>Configure custom queryables for STAC API search using <code>pypgstac load-queryables</code>. Queryables can be loaded from files in the chart or from external ConfigMaps.</p>"},{"location":"deployment/kubernetes/configuration/#basic-configuration","title":"Basic Configuration","text":"<p>Each queryable requires a <code>name</code> field and either a <code>file</code> (from chart) or <code>configMapRef</code> (external ConfigMap):</p> <pre><code>pgstacBootstrap:\n  settings:\n    queryables:\n      # File-based queryable from chart\n      - name: \"common-queryables.json\"\n        file: \"initdb-data/queryables/test-queryables.json\"\n\n      # External ConfigMap reference\n      - name: \"custom-queryables.json\"\n        configMapRef:\n          name: my-custom-queryables-configmap\n          key: queryables.json\n</code></pre>"},{"location":"deployment/kubernetes/configuration/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Description Required Example <code>name</code> Name for the queryables file Yes \"common-queryables.json\" <code>file</code> Path to queryables file in chart No* \"initdb-data/queryables/test-queryables.json\" <code>configMapRef.name</code> Name of external ConfigMap No* \"my-queryables-cm\" <code>configMapRef.key</code> Key in the ConfigMap No* \"queryables.json\" <code>indexFields</code> Fields to create indexes for No [\"platform\", \"instruments\"] <code>deleteMissing</code> Delete queryables not in this file No true <code>collections</code> Apply to specific collections No [\"collection-1\", \"collection-2\"] <p>* Either <code>file</code> or <code>configMapRef</code> must be provided</p>"},{"location":"deployment/kubernetes/configuration/#advanced-example","title":"Advanced Example","text":"<p>Mix file-based and ConfigMap-based queryables with optional parameters:</p> <pre><code>pgstacBootstrap:\n  settings:\n    queryables:\n      # Standard queryables from chart with indexes\n      - name: \"common-queryables.json\"\n        file: \"initdb-data/queryables/common-queryables.json\"\n        indexFields: [\"platform\", \"instruments\"]\n        deleteMissing: true\n\n      # Custom queryables from external ConfigMap\n      - name: \"sentinel-queryables.json\"\n        configMapRef:\n          name: sentinel-queryables-cm\n          key: queryables.json\n        indexFields: [\"sat:orbit_state\", \"sar:instrument_mode\"]\n        collections: [\"sentinel-1-grd\"]\n\n      # Collection-specific queryables\n      - name: \"landsat-queryables.json\"\n        configMapRef:\n          name: landsat-queryables-cm\n          key: data.json\n        collections: [\"landsat-c2-l2\"]\n</code></pre>"},{"location":"deployment/kubernetes/configuration/#external-configmap-setup","title":"External ConfigMap Setup","text":"<p>When using <code>configMapRef</code>, create the ConfigMap separately:</p> <pre><code># From a file\nkubectl create configmap my-queryables-cm \\\n  --from-file=queryables.json=./my-queryables.json \\\n  -n eoapi\n\n# From literal JSON\nkubectl create configmap my-queryables-cm \\\n  --from-literal=queryables.json='{\"$schema\": \"...\", \"properties\": {...}}' \\\n  -n eoapi\n</code></pre> <p>The queryables will be automatically loaded during the PgSTAC bootstrap process.</p>"},{"location":"deployment/kubernetes/configuration/#cloud-storage-authentication","title":"Cloud Storage Authentication","text":"<p>eoAPI services access COG files in cloud storage buckets. Use cloud-native authentication instead of long-lived credentials:</p>"},{"location":"deployment/kubernetes/configuration/#aws-irsa","title":"AWS (IRSA)","text":"<pre><code>serviceAccount:\n  create: true\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/eoapi-s3-access\n</code></pre> <p>The raster service automatically uses IRSA credentials via the AWS SDK credential chain.</p>"},{"location":"deployment/kubernetes/configuration/#azure-workload-identity","title":"Azure (Workload Identity)","text":"<pre><code>serviceAccount:\n  create: true\n  annotations:\n    azure.workload.identity/client-id: \"your-client-id\"\n    azure.workload.identity/tenant-id: \"your-tenant-id\"\n</code></pre>"},{"location":"deployment/kubernetes/configuration/#gcp-workload-identity","title":"GCP (Workload Identity)","text":"<pre><code>serviceAccount:\n  create: true\n  annotations:\n    iam.gke.io/gcp-service-account: eoapi-gcs-sa@project.iam.gserviceaccount.com\n</code></pre> <p>All services using GDAL (raster API with titiler-pgstac) automatically use these credentials through their respective cloud SDKs. No environment variables or hardcoded credentials needed.</p>"},{"location":"deployment/kubernetes/configuration/#ingress-configuration","title":"Ingress Configuration","text":"<p>Unified ingress configuration supporting both NGINX and Traefik:</p> Values Key Description Default Choices <code>ingress.enabled</code> Enable ingress true true/false <code>ingress.className</code> Ingress controller \"nginx\" \"nginx\", \"traefik\" <code>ingress.host</code> Ingress hostname \"\" valid hostname <code>ingress.rootPath</code> Doc server root path \"\" valid path <p>See Unified Ingress Configuration for detailed setup.</p>"},{"location":"deployment/kubernetes/configuration/#service-configuration","title":"Service Configuration","text":"<p>Each service (stac, raster, vector, multidim) supports:</p> Values Key Description Default Choices <code>{service}.enabled</code> Enable the service varies true/false <code>{service}.image.name</code> Container image varies valid image <code>{service}.image.tag</code> Image tag varies valid tag <code>{service}.autoscaling.enabled</code> Enable HPA false true/false <code>{service}.autoscaling.type</code> Scaling metric \"requestRate\" \"cpu\", \"requestRate\", \"both\" <p>Example service configuration: <pre><code>raster:\n  enabled: true\n  autoscaling:\n    enabled: true\n    minReplicas: 2\n    maxReplicas: 10\n    type: \"requestRate\"\n    targets:\n      cpu: 75\n      requestRate: \"100000m\"\n</code></pre></p>"},{"location":"deployment/kubernetes/configuration/#stac-browser","title":"STAC Browser","text":"Values Key Description Default Choices <code>browser.enabled</code> Enable STAC browser true true/false <code>browser.replicaCount</code> Number of replicas 1 integer &gt; 0 <code>browser.ingress.enabled</code> Enable browser ingress true true/false"},{"location":"deployment/kubernetes/configuration/#deployment-architecture","title":"Deployment Architecture","text":"<p>When using default settings, the deployment looks like this: </p> <p>The deployment includes: - HA PostgreSQL database (via PostgreSQL Operator) - Sample data fixtures - Load balancer with path-based routing:   - <code>/stac</code> \u2192 STAC API   - <code>/raster</code> \u2192 Titiler   - <code>/vector</code> \u2192 TiPG   - <code>/browser</code> \u2192 STAC Browser   - <code>/</code> \u2192 Documentation</p>"},{"location":"deployment/kubernetes/configuration/#health-monitoring","title":"Health Monitoring","text":"<p>All services include health check endpoints with automatic liveness probes:</p> Service Health Endpoint Response STAC API <code>/stac/_mgmt/ping</code> HTTP 200, no auth required Raster API <code>/raster/healthz</code> HTTP 200, no auth required Vector API <code>/vector/healthz</code> HTTP 200, no auth required <p>The Kubernetes deployment templates automatically configure <code>livenessProbe</code> settings for regular health checks. See the deployment template for probe configuration details.</p>"},{"location":"deployment/kubernetes/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"deployment/kubernetes/configuration/#autoscaling-behavior","title":"Autoscaling Behavior","text":"<p>Fine-tune scaling behavior:</p> <pre><code>autoscaling:\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n</code></pre> <p>See Kubernetes HPA documentation for details.</p>"},{"location":"deployment/kubernetes/configuration/#resource-requirements","title":"Resource Requirements","text":"<p>Each service can have custom resource limits:</p> <pre><code>settings:\n  resources:\n    limits:\n      cpu: \"768m\"\n      memory: \"1024Mi\"\n    requests:\n      cpu: \"256m\"\n      memory: \"512Mi\"\n</code></pre>"},{"location":"deployment/kubernetes/configuration/#additional-service-settings","title":"Additional Service Settings","text":"<p>Each service also supports: <pre><code>settings:\n  labels: {}                # Additional pod labels\n  extraEnvFrom: []         # Additional environment variables from references\n  extraVolumeMounts: []    # Additional volume mounts\n  extraVolumes: []         # Additional volumes\n  envVars: {}             # Environment variables\n</code></pre></p>"},{"location":"deployment/kubernetes/gcp-gke/","title":"GCP GKE Cluster Setup","text":"<p>This is a verbose walkthrough. It uses <code>gcloud</code> and assumes you already have an GCP account and project where you want to run eoapi. We also assume that you have some prerequisites installed including <code>gcloud</code>, <code>kubectl</code> and <code>helm</code>.</p> <p>If you're familiar with Terraform and would like an IaC choice that is more terse consider setting up your cluster with that: developmentseed/eoapi-k8s-terraform</p>"},{"location":"deployment/kubernetes/gcp-gke/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Pre-requisites</li> <li>Enable GKE API</li> <li>Create GKE k8s Cluster</li> <li>Enable CSI Driver</li> <li>Install NGINX Ingress Controller</li> <li>Install Cert Manager</li> </ul> <p># Pre-requisites</p> <p>Before we begin, make sure you are logged in to your GCP account and have set up a project. You can do this by running the following commands:  <code>gcloud auth login     gcloud config set project &lt;project-name&gt;</code></p>"},{"location":"deployment/kubernetes/gcp-gke/#enable-gke-api","title":"Enable GKE API","text":"<p>Before we can create a cluster, we need to enable the GKE API. You can do this by running the following command: <pre><code>  gcloud services enable container.googleapis.com\n</code></pre></p>"},{"location":"deployment/kubernetes/gcp-gke/#create-gke-k8s-cluster","title":"Create GKE k8s Cluster","text":"<p>Here's an example command to create a cluster. See the gcloud docs for all available options   <pre><code>gcloud container clusters create sandbox \\\n--num-nodes=1 \\\n--zone=us-central1-a \\\n--node-locations=us-central1-a \\\n--enable-autoscaling \\\n--min-nodes=1 \\\n--max-nodes=3 \\\n--machine-type=n1-standard-2\n</code></pre></p> <p>You might need to iterate on the command above, so to delete the cluster:   <pre><code>gcloud container clusters delete sandbox --zone=us-central1-a\n</code></pre></p>"},{"location":"deployment/kubernetes/gcp-gke/#enable-csi-driver","title":"Enable CSI Driver","text":"<p>CSI Driver is required for persistent volumes to be mounted to pods. You can enable it by running the following command:</p> <pre><code>gcloud container clusters update sandbox --update-addons=GcePersistentDiskCsiDriver=ENABLED --zone=us-central1-a\n</code></pre>"},{"location":"deployment/kubernetes/gcp-gke/#connect-to-your-cluster","title":"Connect to Your Cluster","text":"<p>See GKE docs for installing and authenticating with kubectl</p> <p>See docs for installing Helm</p> <p>Configure kubectl to connect to your GKE cluster:</p> <pre><code>gcloud container clusters get-credentials sandbox \\\n    --region=us-central1-a\n</code></pre> <p>To test the connection, run:</p> <pre><code>kubectl get nodes\n</code></pre> <p>You should see something like:</p> <pre><code>NAME                                        STATUS   ROLES    AGE   VERSION\ngke-eoapi-test-default-pool-94a2b7e7-vg4f   Ready    &lt;none&gt;   5m   v1.31.5-gke.1233000\n</code></pre>"},{"location":"deployment/kubernetes/gcp-gke/#install-nginx-ingress-controller","title":"Install NGINX Ingress Controller","text":"<p>NGINX Ingress Controller can be installed through <code>helm</code> using the following command: <pre><code>kubectl create ns eoapi\n\nhelm upgrade --install ingress-nginx ingress-nginx \\\n  --repo https://kubernetes.github.io/ingress-nginx \\\n  --namespace eoapi\n</code></pre></p> <p>See the NGINX Ingress Controller docs for more details and configuration options.</p>"},{"location":"deployment/kubernetes/gcp-gke/#install-cert-manager","title":"Install Cert Manager","text":"<p>Cert Manager can be installed through <code>helm</code> using the following commands: <pre><code>helm repo add jetstack https://charts.jetstack.io\n\nhelm repo update\n\nhelm upgrade --install cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.12.0 \\\n  --set installCRDs=true\n</code></pre></p> <p>Now we are ready to install eoapi. See the eoapi installation instructions for more details.</p>"},{"location":"deployment/kubernetes/gcp-gke/#configure-workload-identity-for-gcs-bucket-access","title":"Configure Workload Identity for GCS Bucket Access","text":"<p>eoAPI services need to access COG files in GCS buckets. Use Workload Identity for secure, temporary credential access instead of long-lived credentials:</p> <ol> <li> <p>Enable Workload Identity on your cluster (if not already enabled):    <pre><code>gcloud container clusters update sandbox \\\n    --workload-pool=PROJECT_ID.svc.id.goog \\\n    --zone=us-central1-a\n</code></pre></p> </li> <li> <p>Create a Google Service Account:    <pre><code>gcloud iam service-accounts create eoapi-gcs-sa \\\n    --display-name=\"eoAPI GCS Service Account\"\n</code></pre></p> </li> <li> <p>Grant GCS permissions to the service account:    <pre><code># For specific bucket access\ngsutil iam ch serviceAccount:eoapi-gcs-sa@PROJECT_ID.iam.gserviceaccount.com:objectViewer gs://your-bucket-name\n\n# Or use IAM roles for multiple buckets\ngcloud projects add-iam-policy-binding PROJECT_ID \\\n    --member=\"serviceAccount:eoapi-gcs-sa@PROJECT_ID.iam.gserviceaccount.com\" \\\n    --role=\"roles/storage.objectViewer\"\n</code></pre></p> </li> <li> <p>Create Kubernetes service account and bind it:    <pre><code>kubectl create serviceaccount eoapi-sa -n eoapi\n\ngcloud iam service-accounts add-iam-policy-binding eoapi-gcs-sa@PROJECT_ID.iam.gserviceaccount.com \\\n    --role roles/iam.workloadIdentityUser \\\n    --member \"serviceAccount:PROJECT_ID.svc.id.goog[eoapi/eoapi-sa]\"\n\nkubectl annotate serviceaccount eoapi-sa -n eoapi \\\n    iam.gke.io/gcp-service-account=eoapi-gcs-sa@PROJECT_ID.iam.gserviceaccount.com\n</code></pre></p> </li> <li> <p>Configure eoAPI in your <code>values.yaml</code>:    <pre><code>serviceAccount:\n  create: false  # We already created it\n  name: eoapi-sa\n</code></pre></p> </li> </ol> <p>The raster service will automatically use Workload Identity credentials. No hardcoded credentials needed!</p>"},{"location":"deployment/kubernetes/helm-install/","title":"Manual Helm Install","text":"<ol> <li><code>eoapi-k8s</code> depends on the Crunchydata Postgresql Operator. Install that first:</li> </ol> <pre><code># Check latest version at: https://github.com/CrunchyData/postgres-operator/releases\n$ helm install --set disable_check_for_upgrades=true pgo oci://registry.developers.crunchydata.com/crunchydata/pgo --version 5.7.0\n</code></pre> <ol> <li> <p>Add the eoapi repo from devseed.com/eoapi-k8s/:</p> <pre><code>$ helm repo add eoapi https://devseed.com/eoapi-k8s/\n</code></pre> </li> <li> <p>List out the eoapi chart versions</p> </li> </ol> <pre><code>$ helm search repo eoapi --versions\n# Use latest stable version from output above\n</code></pre> <ol> <li>Optionally override keys/values in the default <code>values.yaml</code> with a custom <code>config.yaml</code> like below:</li> </ol> <pre><code>$ cat config.yaml\nvector:\n  enable: false\npgstacBootstrap:\n  settings:\n    envVars:\n      LOAD_FIXTURES: \"0\"\n      RUN_FOREVER: \"1\"\n</code></pre> <ol> <li>Then <code>helm install</code> with those <code>config.yaml</code> values:</li> </ol> <pre><code># Replace VERSION with latest from `helm search repo eoapi`\n$ export CHART_VERSION=$(helm search repo eoapi/eoapi --versions | head -2 | tail -1 | awk '{print $2}')\n$ helm install -n eoapi --create-namespace eoapi eoapi/eoapi --version $CHART_VERSION -f config.yaml\n</code></pre> <ol> <li> <p>or check out this repo and <code>helm install</code> from this repo's <code>charts/</code> folder:</p> <pre><code>  ######################################################\n  # create os environment variables for required secrets\n  ######################################################\n  $ export GITSHA=$(git rev-parse HEAD | cut -c1-10)\n\n  $ cd ./charts\n\n  $ helm install \\\n      --namespace eoapi \\\n      --create-namespace \\\n      --set gitSha=$GITSHA \\\n      eoapi \\\n      ./eoapi\n</code></pre> </li> </ol>"},{"location":"deployment/kubernetes/manage-data/","title":"Data management","text":"<p>eoAPI-k8s provides a basic data ingestion process that consist of manual operations on the components of the stack.</p>"},{"location":"deployment/kubernetes/manage-data/#load-data","title":"Load data","text":"<p>You will have to have STAC records for the collection and items you wish to load (e.g., <code>collections.json</code> and <code>items.json</code>). This repo contains a few script that may help you to generate sample input data.</p>"},{"location":"deployment/kubernetes/manage-data/#preshipped-bash-script","title":"Preshipped bash script","text":"<p>Execute <code>make ingest</code> to load data into the eoAPI service - it expects <code>collections.json</code> and <code>items.json</code> in the current directory.</p>"},{"location":"deployment/kubernetes/manage-data/#manual-steps","title":"Manual steps","text":"<p>In order to add raster data to eoAPI you can load STAC collections and items into the PostgreSQL database using pgSTAC and the tool <code>pypgstac</code>.</p> <p>First, ensure your Kubernetes cluster is running and <code>kubectl</code> is configured to access and modify it.</p> <p>In a second step, you'll have to upload the data into the pod running the raster eoAPI service. You can use the following commands to copy the data:</p> <p><pre><code>kubectl cp collections.json \"$NAMESPACE/$EOAPI_POD_RASTER\":/tmp/collections.json\nkubectl cp items.json \"$NAMESPACE/$EOAPI_POD_RASTER\":/tmp/items.json\n</code></pre> Then, bash into the pod or server running the raster eoAPI service, you can use the following commands to load the data:</p> <pre><code>#!/bin/bash\napt update -y &amp;&amp; apt install python3 python3-pip -y &amp;&amp; pip install pypgstac[psycopg]';\npypgstac pgready --dsn $PGADMIN_URI\npypgstac load collections /tmp/collections.json --dsn $PGADMIN_URI --method insert_ignore\npypgstac load items /tmp/items.json --dsn $PGADMIN_URI --method insert_ignore\n</code></pre>"},{"location":"deployment/kubernetes/observability/","title":"Observability","text":"<p># Observability &amp; Monitoring</p> <p>This guide covers metrics collection, monitoring, and visualization for eoAPI deployments. All monitoring components are optional and disabled by default.</p>"},{"location":"deployment/kubernetes/observability/#overview","title":"Overview","text":"<p>eoAPI observability is implemented through conditional dependencies in the main <code>eoapi</code> chart:</p>"},{"location":"deployment/kubernetes/observability/#core-monitoring","title":"Core Monitoring","text":"<p>Essential metrics collection infrastructure including Prometheus server, metrics-server, kube-state-metrics, node-exporter, and prometheus-adapter.</p>"},{"location":"deployment/kubernetes/observability/#integrated-observability","title":"Integrated Observability","text":"<p>Grafana dashboards and visualization tools are available as conditional dependencies within the main chart, eliminating the need for separate deployments.</p>"},{"location":"deployment/kubernetes/observability/#configuration","title":"Configuration","text":"<p>Prerequisites: Kubernetes cluster with Helm 3 installed.</p>"},{"location":"deployment/kubernetes/observability/#quick-deployment","title":"Quick Deployment","text":"<pre><code># Deploy with monitoring and observability enabled\nhelm install eoapi eoapi/eoapi \\\n  --set monitoring.prometheus.enabled=true \\\n  --set observability.grafana.enabled=true\n\n# Access Grafana (see \"Accessing Grafana\" section below for credentials)\nkubectl port-forward -n eoapi svc/eoapi-obs-grafana 3000:80\n</code></pre>"},{"location":"deployment/kubernetes/observability/#using-configuration-files","title":"Using Configuration Files","text":"<p>For production deployments, use configuration files instead of command-line flags:</p> <pre><code># Deploy with integrated monitoring and observability\nhelm install eoapi eoapi/eoapi -f values-full-observability.yaml\n</code></pre> <p>For a complete example: See production profile</p>"},{"location":"deployment/kubernetes/observability/#architecture-components","title":"Architecture &amp; Components","text":"<p>Component Responsibilities:</p> <ul> <li>Prometheus Server: Central metrics storage and querying engine</li> <li>metrics-server: Provides resource metrics for <code>kubectl top</code> and HPA</li> <li>kube-state-metrics: Exposes Kubernetes object state as metrics</li> <li>prometheus-node-exporter: Collects hardware and OS metrics from nodes</li> <li>prometheus-adapter: Enables custom metrics for Horizontal Pod Autoscaler</li> <li>Grafana: Dashboards and visualization of collected metrics</li> </ul> <p>Data Flow: Exporters expose metrics \u2192 Prometheus scrapes and stores \u2192 Grafana/kubectl query via PromQL \u2192 Dashboards visualize data</p>"},{"location":"deployment/kubernetes/observability/#detailed-configuration","title":"Detailed Configuration","text":""},{"location":"deployment/kubernetes/observability/#basic-monitoring-setup","title":"Basic Monitoring Setup","text":"<pre><code># values.yaml - Enable core monitoring in main eoapi chart\nmonitoring:\n  metricsServer:\n    enabled: true\n  prometheus:\n    enabled: true\n    server:\n      persistentVolume:\n        enabled: true\n        size: 50Gi\n      retention: \"30d\"\n    kube-state-metrics:\n      enabled: true\n    prometheus-node-exporter:\n      enabled: true\n</code></pre>"},{"location":"deployment/kubernetes/observability/#observability-chart-configuration","title":"Observability Chart Configuration","text":"<pre><code># Basic Grafana setup\ngrafana:\n  enabled: true\n  service:\n    type: LoadBalancer\n\n# Connect to external Prometheus (if not using eoapi's Prometheus)\nprometheusUrl: \"http://prometheus.monitoring.svc.cluster.local\"\n\n# Production Grafana configuration\ngrafana:\n  persistence:\n    enabled: true\n    size: 10Gi\n  resources:\n    limits:\n      cpu: 200m\n      memory: 400Mi\n    requests:\n      cpu: 50m\n      memory: 200Mi\n</code></pre>"},{"location":"deployment/kubernetes/observability/#postgresql-monitoring","title":"PostgreSQL Monitoring","text":"<p>Enable PostgreSQL metrics collection:</p> <pre><code>postgrescluster:\n  monitoring: true  # Enables postgres_exporter sidecar\n</code></pre>"},{"location":"deployment/kubernetes/observability/#available-metrics","title":"Available Metrics","text":""},{"location":"deployment/kubernetes/observability/#core-infrastructure-metrics","title":"Core Infrastructure Metrics","text":"<ul> <li>Container resources: CPU, memory, network usage</li> <li>Kubernetes state: Pods, services, deployments status</li> <li>Node metrics: Hardware utilization, filesystem usage</li> <li>PostgreSQL: Database connections, query performance (when enabled)</li> </ul>"},{"location":"deployment/kubernetes/observability/#custom-application-metrics","title":"Custom Application Metrics","text":"<p>When prometheus-adapter and nginx ingress are both enabled, these custom metrics become available: - <code>nginx_ingress_controller_requests_rate_stac_eoapi</code> - <code>nginx_ingress_controller_requests_rate_raster_eoapi</code> - <code>nginx_ingress_controller_requests_rate_vector_eoapi</code> - <code>nginx_ingress_controller_requests_rate_multidim_eoapi</code></p> <p>Requirements: - nginx ingress controller with prometheus metrics enabled - Ingress must use specific hostnames (not wildcard patterns) - prometheus-adapter must be configured to expose these metrics</p>"},{"location":"deployment/kubernetes/observability/#pre-built-dashboards","title":"Pre-built Dashboards","text":"<p>The <code>eoapi-observability</code> chart provides ready-to-use dashboards:</p>"},{"location":"deployment/kubernetes/observability/#eoapi-services-dashboard","title":"eoAPI Services Dashboard","text":"<ul> <li>Request rates per service</li> <li>Response times and error rates</li> <li>Traffic patterns by endpoint</li> </ul>"},{"location":"deployment/kubernetes/observability/#infrastructure-dashboard","title":"Infrastructure Dashboard","text":"<ul> <li>CPU usage rate by pod</li> <li>CPU throttling metrics</li> <li>Memory usage and limits</li> <li>Pod count tracking</li> </ul>"},{"location":"deployment/kubernetes/observability/#container-resources-dashboard","title":"Container Resources Dashboard","text":"<ul> <li>Resource consumption by container</li> <li>Resource quotas and limits</li> <li>Performance bottlenecks</li> </ul>"},{"location":"deployment/kubernetes/observability/#postgresql-dashboard-when-enabled","title":"PostgreSQL Dashboard (when enabled)","text":"<ul> <li>Database connections</li> <li>Query performance</li> <li>Storage utilization</li> </ul>"},{"location":"deployment/kubernetes/observability/#production-configuration","title":"Production Configuration","text":"<pre><code>monitoring:\n  prometheus:\n    server:\n      # Persistent storage\n      persistentVolume:\n        enabled: true\n        size: 100Gi\n        storageClass: \"gp3\"\n      # Retention policy\n      retention: \"30d\"\n      # Resource allocation\n      resources:\n        limits:\n          cpu: \"2000m\"\n          memory: \"4096Mi\"\n        requests:\n          cpu: \"1000m\"\n          memory: \"2048Mi\"\n      # Security - internal access only\n      service:\n        type: ClusterIP\n</code></pre>"},{"location":"deployment/kubernetes/observability/#resource-requirements","title":"Resource Requirements","text":""},{"location":"deployment/kubernetes/observability/#core-monitoring-components","title":"Core Monitoring Components","text":"<p>Minimum resource requirements (actual usage varies by cluster size and metrics volume):</p> Component CPU Memory Purpose prometheus-server 500m 1Gi Metrics storage metrics-server 100m 200Mi Resource metrics kube-state-metrics 50m 150Mi K8s state prometheus-node-exporter 50m 50Mi Node metrics prometheus-adapter 100m 128Mi Custom metrics API Total ~800m ~1.5Gi"},{"location":"deployment/kubernetes/observability/#observability-components","title":"Observability Components","text":"Component CPU Memory Purpose grafana 100m 200Mi Visualization"},{"location":"deployment/kubernetes/observability/#operations","title":"Operations","text":""},{"location":"deployment/kubernetes/observability/#accessing-grafana","title":"Accessing Grafana","text":"<pre><code># Get Grafana admin username (usually 'admin')\nkubectl get secret -n eoapi -l app.kubernetes.io/name=grafana \\\n  -o jsonpath=\"{.items[0].data.admin-user}\" | base64 --decode\n\n# Get Grafana admin password\nkubectl get secret -n eoapi -l app.kubernetes.io/name=grafana \\\n  -o jsonpath=\"{.items[0].data.admin-password}\" | base64 --decode\n\n# Port-forward to access Grafana UI\nkubectl port-forward -n eoapi svc/eoapi-obs-grafana 3000:80\n# Access at http://localhost:3000\n</code></pre>"},{"location":"deployment/kubernetes/observability/#verification-commands","title":"Verification Commands","text":"<pre><code># Check Prometheus is running\nkubectl get pods -n eoapi -l app.kubernetes.io/name=prometheus\n\n# Verify metrics-server\nkubectl get apiservice v1beta1.metrics.k8s.io\n\n# List available custom metrics\nkubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1\" | jq '.resources[].name'\n\n# Test metrics collection\nkubectl port-forward svc/eoapi-prometheus-server 9090:80 -n eoapi\n# Visit http://localhost:9090/targets\n</code></pre>"},{"location":"deployment/kubernetes/observability/#monitoring-health","title":"Monitoring Health","text":"<pre><code># Check Prometheus targets\ncurl -X GET 'http://localhost:9090/api/v1/query?query=up'\n\n# Verify Grafana datasource connectivity\nkubectl exec -it deployment/eoapi-obs-grafana -n eoapi -- \\\n  wget -O- http://eoapi-prometheus-server/api/v1/label/__name__/values\n</code></pre>"},{"location":"deployment/kubernetes/observability/#advanced-features","title":"Advanced Features","text":""},{"location":"deployment/kubernetes/observability/#alerting-setup","title":"Alerting Setup","text":"<p>Enable alertmanager for alert management:</p> <pre><code>prometheus:\n  enabled: true\n  alertmanager:\n    enabled: true\n    config:\n      global:\n        # Configure with your SMTP server details\n        smtp_smarthost: 'your-smtp-server:587'\n        smtp_from: 'alertmanager@yourdomain.com'\n      route:\n        receiver: 'default-receiver'\n      receivers:\n      - name: 'default-receiver'\n        webhook_configs:\n        - url: 'http://your-webhook-endpoint:5001/'\n</code></pre> <p>Note: Replace example values with your actual SMTP server and webhook endpoints.</p>"},{"location":"deployment/kubernetes/observability/#batch-job-metrics","title":"Batch Job Metrics","text":"<p>Enable pushgateway for batch job metrics:</p> <pre><code>prometheus:\n  enabled: true\n  prometheus-pushgateway:\n    enabled: true  # For batch job metrics collection\n</code></pre>"},{"location":"deployment/kubernetes/observability/#custom-dashboards","title":"Custom Dashboards","text":"<p>Add custom dashboards by creating ConfigMaps with the appropriate label:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: custom-dashboard\n  namespace: eoapi\n  labels:\n    eoapi_dashboard: \"1\"\ndata:\n  custom.json: |\n    {\n      \"dashboard\": {\n        \"id\": null,\n        \"title\": \"Custom eoAPI Dashboard\",\n        \"tags\": [\"eoapi\"],\n        \"panels\": []\n      }\n    }\n</code></pre> <p>The ConfigMap must be in the same namespace as the Grafana deployment and include the <code>eoapi_dashboard: \"1\"</code> label.</p>"},{"location":"deployment/kubernetes/observability/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/kubernetes/observability/#common-issues","title":"Common Issues","text":"<p>Missing Metrics 1. Check Prometheus service discovery:    <pre><code>kubectl port-forward svc/eoapi-prometheus-server 9090:80 -n eoapi\n# Visit http://localhost:9090/service-discovery\n</code></pre></p> <ol> <li>Verify target endpoints:    <pre><code>kubectl get endpoints -n eoapi\n</code></pre></li> </ol> <p>Grafana Connection Issues 1. Check datasource connectivity in Grafana UI \u2192 Configuration \u2192 Data Sources 2. Verify Prometheus URL accessibility from Grafana pod</p> <p>Resource Issues - Monitor current usage: <code>kubectl top pods -n eoapi</code> - Check for OOMKilled containers: <code>kubectl describe pods -n eoapi | grep -A 5 \"Last State\"</code> - Verify resource limits are appropriate for your workload size - Consider increasing Prometheus retention settings if storage is full</p>"},{"location":"deployment/kubernetes/observability/#security-considerations","title":"Security Considerations","text":"<ul> <li>Network Security: Use <code>ClusterIP</code> services for Prometheus in production</li> <li>Access Control: Configure network policies to restrict metrics access</li> <li>Authentication: Enable authentication for Grafana (LDAP, OAuth, etc.)</li> <li>Data Privacy: Consider metrics data sensitivity and retention policies</li> </ul>"},{"location":"deployment/kubernetes/observability/#related-documentation","title":"Related Documentation","text":"<ul> <li>For autoscaling configuration using these metrics: autoscaling.md</li> </ul>"},{"location":"deployment/kubernetes/quick-start/","title":"Quick Start","text":""},{"location":"deployment/kubernetes/quick-start/#prerequisites","title":"Prerequisites","text":"<ul> <li>helm</li> <li>A Kubernetes cluster (local or cloud-based)</li> <li><code>kubectl</code> configured for your cluster (ensure <code>KUBECONFIG</code> environment variable is set to point to your cluster configuration file, or use <code>kubectl config use-context &lt;your-context&gt;</code> to set the active cluster)</li> <li>helm unittest if contributing to the repository and running <code>./eoapi-cli test unit</code></li> </ul>"},{"location":"deployment/kubernetes/quick-start/#option-1-one-command-installation","title":"Option 1: One-Command Installation","text":"<p>The fastest way to get started is using our eoAPI CLI:</p> <p>For local development with k3s/k3d: <pre><code>./eoapi-cli cluster start\n./eoapi-cli deployment run\n</code></pre></p> <p>For cloud deployment: <pre><code>./eoapi-cli deployment run\n</code></pre></p> <p>This will automatically: 1. Install the PostgreSQL operator 2. Add the eoAPI helm repository 3. Install the eoAPI helm chart 4. Set up necessary namespaces and configurations</p> <p>[!WARNING] Some images do not provide a <code>linux/arm64</code> compatible download (You may see image pull failures) which causes failures on M1 etc Macs, to get around this, you can pre-pull the image with: <pre><code>docker pull --platform=linux/amd64 &lt;image&gt;\nminikube image load &lt;image&gt;\n</code></pre> You can then re-deploy the service and it will now use the local image.</p>"},{"location":"deployment/kubernetes/quick-start/#option-2-step-by-step-installation","title":"Option 2: Step-by-Step Installation","text":"<p>If you prefer more control over the installation process:</p> <ol> <li> <p>Install the PostgreSQL operator: <pre><code>helm upgrade --install \\\n  --set disable_check_for_upgrades=true pgo \\\n  oci://registry.developers.crunchydata.com/crunchydata/pgo \\\n  --version 5.7.4\n</code></pre></p> </li> <li> <p>Add the eoAPI helm repository: <pre><code>helm repo add eoapi https://devseed.com/eoapi-k8s/\n</code></pre></p> </li> <li> <p>Get your current git SHA: <pre><code>export GITSHA=$(git rev-parse HEAD | cut -c1-10)\n</code></pre></p> </li> <li> <p>Install eoAPI: <pre><code>helm upgrade --install \\\n  --namespace eoapi \\\n  --create-namespace \\\n  --set gitSha=$GITSHA \\\n  eoapi devseed/eoapi\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/quick-start/#post-installation","title":"Post-Installation","text":"<ol> <li> <p>Enable ingress (for Minikube only - k3s has Traefik built-in): <pre><code>minikube addons enable ingress\n</code></pre></p> </li> <li> <p>Optional: Load sample data: <pre><code>./eoapi-cli ingest collections.json items.json\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/release/","title":"Release Workflow","text":"<ol> <li> <p>PRs that include changes in the <code>charts/&lt;eoapi&gt; || &lt;eoapi-support&gt; || &lt;postgrescluster&gt;</code> charts are manually required to consider whether their changes are major, minor or patch (in terms of semantic versioning) and bump the appropriate chart <code>version:</code> (which follows semver) and <code>appVersion:</code> (which does not follow semver) for each affected chart</p> </li> <li> <p>The releaser then merges the above PR</p> </li> <li> <p>Then the releaser should go to the Github release UI/UX and kick off a new release by doing the following:</p> </li> <li> <p>click \"Draft New Release\"</p> </li> <li> <p>create a new tag increment based on the last one that matches the pattern <code>v&lt;major&gt;.&lt;minor&gt;.&lt;patch&gt;</code>. This does not have to match any of the chart versions you changed in the above PR. This repository is one-to-many with charts. So in terms of GH release we are saying, \"we've release one of the three charts above\" and the commit message will reflect that</p> </li> <li> <p>click the \"Generate release notes\"</p> </li> <li> <p>review the release notes and clean up and makes sure talk about which chart you released</p> </li> <li> <p>click the \"Publish release\"</p> </li> <li> <p>This last step then kicks off another GH Actions workflow called \"release.yaml\" which publishes any helm charts that had version bumps since the last time</p> </li> <li> <p>Verify the release is all good by running <code>helm repo update &amp;&amp; helm search repo eoapi --versions</code></p> </li> </ol>"},{"location":"deployment/kubernetes/stac-auth-proxy/","title":"STAC Auth Proxy","text":""},{"location":"deployment/kubernetes/stac-auth-proxy/#solution-overview","title":"Solution Overview","text":"<p>We have implemented support for STAC Auth Proxy integration with eoAPI-K8S through service-specific ingress control. This feature allows the STAC service to be accessible only internally while other services remain externally available.</p>"},{"location":"deployment/kubernetes/stac-auth-proxy/#implementation-details","title":"Implementation Details","text":""},{"location":"deployment/kubernetes/stac-auth-proxy/#1-service-specific-ingress-control","title":"1. Service-Specific Ingress Control","text":"<p>Each service can now independently control its ingress settings via the values.yaml configuration:</p> <pre><code>stac:\n  enabled: true\n  ingress:\n    enabled: false  # Disable external ingress for STAC only\n\n# Other services remain externally accessible\nraster:\n  enabled: true\n  ingress:\n    enabled: true\n</code></pre>"},{"location":"deployment/kubernetes/stac-auth-proxy/#deployment-guide","title":"Deployment Guide","text":""},{"location":"deployment/kubernetes/stac-auth-proxy/#1-configure-eoapi-k8s","title":"1. Configure eoAPI-K8S","text":"<pre><code># values.yaml for eoapi-k8s\nstac:\n  enabled: true\n  ingress:\n    enabled: false  # No external ingress for STAC\n\n# Other services remain externally accessible\nraster:\n  enabled: true\nvector:\n  enabled: true\nmultidim:\n  enabled: true\n</code></pre>"},{"location":"deployment/kubernetes/stac-auth-proxy/#2-deploy-stac-auth-proxy","title":"2. Deploy STAC Auth Proxy","text":"<p>Deploy the stac-auth-proxy Helm chart in the same namespace:</p> <pre><code># values.yaml for stac-auth-proxy\nbackend:\n  service: stac  # Internal K8s service name\n  port: 8080     # Service port\n\nauth:\n  # Configure authentication settings\n  provider: oauth2\n  # ... other auth settings\n</code></pre>"},{"location":"deployment/kubernetes/stac-auth-proxy/#3-network-flow","title":"3. Network Flow","text":"<pre><code>graph LR\n    A[External Request] --&gt; B[STAC Auth Proxy]\n    B --&gt;|Authentication| C[Internal STAC Service]\n    D[External Request] --&gt;|Direct Access| E[Raster/Vector/Other Services]\n</code></pre>"},{"location":"deployment/kubernetes/stac-auth-proxy/#testing","title":"Testing","text":"<p>Verify the configuration:</p> <pre><code># Check that STAC paths are excluded\nhelm template eoapi --set stac.ingress.enabled=false,stac.enabled=true -f values.yaml\n\n# Verify other services remain accessible\nkubectl get ingress\nkubectl get services\n</code></pre> <p>Expected behavior: - STAC service accessible only within the cluster - Other services (raster, vector, etc.) accessible via their ingress paths - Auth proxy successfully routing authenticated requests to STAC</p>"},{"location":"deployment/kubernetes/stac-auth-proxy/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>STAC Service Not Accessible Internally</li> <li>Verify service is running: <code>kubectl get services</code></li> <li>Check service port matches auth proxy configuration</li> <li> <p>Verify network policies allow proxy-to-STAC communication</p> </li> <li> <p>Other Services Affected</p> </li> <li>Confirm ingress configuration for other services</li> <li>Check ingress controller logs</li> <li>Verify service-specific settings in values.yaml</li> </ol>"},{"location":"deployment/kubernetes/stac-auth-proxy/#root-path-configuration-for-direct-service-access","title":"Root Path Configuration for Direct Service Access","text":""},{"location":"deployment/kubernetes/stac-auth-proxy/#understanding-usage-of-overriderootpath-with-stac-auth-proxy","title":"Understanding usage of overrideRootPath with stac-auth-proxy","text":"<p>When deploying the eoAPI-K8S with the STAC service behind a stac-auth-proxy, you may want to configure the <code>stac.overrideRootPath</code> parameter to control how the FastAPI application handles root path prefixes. This is particularly useful when the auth proxy is responsible for managing the <code>/stac</code> path prefix.</p> <p>When deploying stac-auth-proxy in front of the eoAPI service, you may need to configure the root path behavior to avoid URL conflicts. The <code>stac.overrideRootPath</code> parameter allows you to control how the STAC FastAPI application handles root path prefixes.</p>"},{"location":"deployment/kubernetes/stac-auth-proxy/#setting-overriderootpath-to-empty-string","title":"Setting overrideRootPath to Empty String","text":"<p>For stac-auth-proxy deployments, you often want to set <code>stac.overrideRootPath</code> to an empty string:</p> <pre><code># values.yaml for eoapi-k8s\nstac:\n  enabled: true\n  overrideRootPath: \"\"  # Empty string removes --root-path argument\n  ingress:\n    enabled: false  # Disable external ingress for STAC\n</code></pre> <p>Important: This configuration creates an intentional inconsistency:</p> <ul> <li>Ingress routes: Still configured for <code>/stac</code> (if ingress was enabled)</li> <li>FastAPI application: Runs without any root path prefix (no <code>--root-path</code> argument)</li> </ul>"},{"location":"deployment/kubernetes/stac-auth-proxy/#why-this-works-for-stac-auth-proxy","title":"Why This Works for stac-auth-proxy","text":"<p>This behavior is specifically designed for stac-auth-proxy scenarios where:</p> <ol> <li>stac-auth-proxy receives requests via its own ingress and handles the <code>/stac</code> path prefix</li> <li>stac-auth-proxy filters requests managing the <code>/stac</code> prefix and forwards them directly to the STAC service without the path prefix</li> <li>STAC service responds from its internal service as if it's running at the root path <code>/</code></li> </ol>"},{"location":"deployment/kubernetes/stac-auth-proxy/#configuration-examples","title":"Configuration Examples","text":""},{"location":"deployment/kubernetes/stac-auth-proxy/#standard-deployment-with-ingress","title":"Standard Deployment (with ingress)","text":"<pre><code>stac:\n  enabled: true\n  # Default behavior - uses ingress.path as root-path\n  ingress:\n    enabled: true\n    path: \"/stac\"\n</code></pre> <p>Result: FastAPI runs with <code>--root-path=/stac</code></p>"},{"location":"deployment/kubernetes/stac-auth-proxy/#stac-auth-proxy-deployment","title":"stac-auth-proxy Deployment","text":"<pre><code>stac:\n  enabled: true\n  overrideRootPath: \"\"  # Empty string - no --root-path argument\n  ingress:\n    enabled: false  # Access via auth proxy only\n</code></pre> <p>Result: FastAPI runs without <code>--root-path</code> argument</p>"},{"location":"deployment/kubernetes/stac-auth-proxy/#custom-root-path","title":"Custom Root Path","text":"<pre><code>stac:\n  enabled: true\n  overrideRootPath: \"/api/v1/stac\"  # Custom path\n</code></pre> <p>Result: FastAPI runs with <code>--root-path=/api/v1/stac</code></p>"},{"location":"deployment/kubernetes/stac-auth-proxy/#request-flow-with-stac-auth-proxy","title":"Request Flow with stac-auth-proxy","text":"<pre><code>graph LR\n    A[Client Request: /stac/collections] --&gt; B[stac-auth-proxy]\n    B --&gt;|Authentication &amp; Authorization| C[Forward: /collections]\n    C --&gt; D[STAC Service: receives /collections]\n    D --&gt; E[Response: collections data]\n    E --&gt; B\n    B --&gt; A\n</code></pre>"},{"location":"deployment/kubernetes/stac-auth-proxy/#additional-notes","title":"Additional Notes","text":"<ul> <li>The solution leverages Kubernetes service discovery for internal communication</li> <li>No changes required to the STAC service itself</li> <li>Zero downtime deployment possible</li> <li>Existing deployments without auth proxy remain compatible</li> <li>The <code>overrideRootPath: \"\"</code> configuration is specifically for proxy scenarios</li> </ul>"},{"location":"deployment/kubernetes/unified-ingress/","title":"Unified Ingress Configuration","text":"<p>This document describes the unified ingress approach implemented in the eoAPI Helm chart.</p>"},{"location":"deployment/kubernetes/unified-ingress/#overview","title":"Overview","text":"<p>eoAPI includes a streamlined ingress configuration with smart defaults for different controllers. This approach:</p> <ul> <li>Eliminates manual pathType and suffix configurations</li> <li>Uses controller-specific optimizations for NGINX and Traefik</li> <li>Provides separate configuration for STAC browser</li> <li>Maintains backward compatibility while improving usability</li> </ul>"},{"location":"deployment/kubernetes/unified-ingress/#configuration","title":"Configuration","text":"<p>The ingress configuration has been simplified in the <code>values.yaml</code> file:</p> <pre><code>ingress:\n  # Unified ingress configuration for both nginx and traefik\n  enabled: true\n  # ingressClassName: \"nginx\" or \"traefik\"\n  className: \"nginx\"\n  # Root path for doc server\n  rootPath: \"\"\n  # Host configuration\n  host: \"\"\n  # Custom annotations to add to the ingress\n  annotations: {}\n  # TLS configuration\n  tls:\n    enabled: false\n    secretName: eoapi-tls\n</code></pre>"},{"location":"deployment/kubernetes/unified-ingress/#controller-specific-behavior","title":"Controller-Specific Behavior","text":""},{"location":"deployment/kubernetes/unified-ingress/#nginx-ingress-controller","title":"NGINX Ingress Controller","text":"<p>For NGINX, the system automatically: - Uses <code>ImplementationSpecific</code> pathType - Adds regex-based path matching - Sets up proper rewrite rules</p> <p>Basic NGINX configuration: <pre><code>ingress:\n  enabled: true\n  className: \"nginx\"\n  annotations:\n    # Additional custom annotations if needed\n    nginx.ingress.kubernetes.io/enable-cors: \"true\"\n    nginx.ingress.kubernetes.io/enable-access-log: \"true\"\n</code></pre></p>"},{"location":"deployment/kubernetes/unified-ingress/#traefik-ingress-controller","title":"Traefik Ingress Controller","text":"<p>For Traefik, the system: - Uses <code>Prefix</code> pathType by default - Automatically configures strip-prefix middleware - Handles path-based routing appropriately</p> <p>Basic Traefik configuration: <pre><code>ingress:\n  enabled: true\n  className: \"traefik\"\n  # When using TLS, setting host is required\n  host: \"example.domain.com\"\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web\n</code></pre></p>"},{"location":"deployment/kubernetes/unified-ingress/#stac-browser-configuration","title":"STAC Browser Configuration","text":"<p>The STAC browser now uses a separate ingress configuration to handle its unique requirements: - Fixed <code>/browser</code> path prefix - Special rewrite rules for browser-specific routes - Maintains compatibility with both NGINX and Traefik</p> <p>The browser-specific ingress is automatically configured when browser is enabled: <pre><code>browser:\n  enabled: true\n  ingress:\n    enabled: true  # Can be disabled independently\n</code></pre></p>"},{"location":"deployment/kubernetes/unified-ingress/#setting-up-tls-with-cert-manager","title":"Setting up TLS with cert-manager","text":"<p>cert-manager can be used to automatically obtain and manage TLS certificates. Here's how to set it up with Let's Encrypt:</p> <ol> <li> <p>First, install cert-manager in your cluster: <pre><code>helm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --set installCRDs=true\n</code></pre></p> </li> <li> <p>Create a ClusterIssuer for Let's Encrypt (staging first for testing): <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-staging\nspec:\n  acme:\n    # Use Let's Encrypt staging environment first\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\n    email: your-email@example.com\n    privateKeySecretRef:\n      name: letsencrypt-staging\n    solvers:\n    - http01:\n        ingress:\n          class: nginx  # or traefik, depending on your setup\n</code></pre></p> </li> <li> <p>After testing with staging, create the production issuer: <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: your-email@example.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx  # or traefik, depending on your setup\n</code></pre></p> </li> <li> <p>Configure your eoAPI ingress to use cert-manager: <pre><code>ingress:\n  enabled: true\n  className: \"nginx\"  # or \"traefik\"\n  host: \"eoapi.example.com\"\n  annotations:\n    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n  tls:\n    enabled: true\n    secretName: eoapi-tls  # cert-manager will create this secret\n</code></pre></p> </li> </ol>"},{"location":"deployment/kubernetes/unified-ingress/#migration-from-070","title":"Migration from 0.7.0","text":"<p>If you're upgrading from version 0.7.0:</p> <ol> <li>Remove any <code>pathType</code> and <code>pathSuffix</code> configurations from your values</li> <li>The system will automatically use the appropriate settings for your chosen controller</li> <li>For NGINX users, regex path matching is now enabled by default</li> <li>For Traefik users, strip-prefix middleware is automatically configured</li> </ol>"},{"location":"deployment/kubernetes/unified-ingress/#path-structure","title":"Path Structure","text":"<p>Default service paths are: - <code>/stac</code> - STAC API - <code>/raster</code> - Raster API - <code>/vector</code> - Vector API - <code>/multidim</code> - Multi-dimensional API - <code>/browser</code> - STAC Browser (separate ingress) - <code>/</code> - Documentation server (when enabled)</p> <p>These paths are automatically configured with the appropriate rewrites for each controller.</p>"}]}